{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"INSTALLING REQUIRED PACKAGES FOR KAGGLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"\n",
    "    Install package using pip if not already installed.\n",
    "    \n",
    "    Handles version-specific packages and gracefully manages\n",
    "    installation failures.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract package name from version specifications\n",
    "        if '>=' in package or '==' in package:\n",
    "            package_name = package.split('>=')[0].split('==')[0]\n",
    "            __import__(package_name)\n",
    "            print(\"Upgrading {}...\".format(package_name))\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"-q\", package])\n",
    "            print(\"OK: {} upgraded\".format(package))\n",
    "        else:\n",
    "            __import__(package.split('[')[0])\n",
    "            print(\"OK: {} already installed\".format(package))\n",
    "    except ImportError:\n",
    "        print(\"Installing {}...\".format(package))\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"-q\", package])\n",
    "            print(\"OK: {} installed\".format(package))\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(\"WARNING: Failed to install {}. Continuing...\".format(package))\n",
    "    except Exception as e:\n",
    "        print(\"WARNING: Error with {}: {}. Continuing...\".format(package, str(e)))\n",
    "\n",
    "# Core packages required for model optimization\n",
    "required_packages = [\n",
    "    \"transformers\",          # Model loading (has BitsAndBytesConfig)\n",
    "    \"peft\",                   # LoRA adapter support\n",
    "    \"accelerate\",            # Distributed model loading\n",
    "    \"torch\",                  # PyTorch (usually pre-installed)\n",
    "    \"faiss-cpu\",                     # Vector search\n",
    "    \"sentence-transformers\",         # Embedding models\n",
    "    \"psutil\",                        # Memory monitoring\n",
    "    \"safetensors>=0.4.0\",           # Safe model serialization\n",
    "    \"bitsandbytes>=0.39.0\",          # 8-bit quantization (more stable)\n",
    "    \"llama-cpp-python\"\n",
    "]\n",
    "\n",
    "print(\"\\nInstalling packages:\\n\")\n",
    "\n",
    "for package in required_packages:\n",
    "    install_package(package)\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICATION OF CRITICAL IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFYING CRITICAL IMPORTS\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "critical_imports = {\n",
    "    \"transformers\": \"BitsAndBytesConfig\",\n",
    "    \"torch\": \"torch version\",\n",
    "    \"peft\": \"PeftModel\",\n",
    "    \"bitsandbytes\": \"bitsandbytes version\",\n",
    "    \"psutil\": \"memory monitoring\"\n",
    "}\n",
    "\n",
    "all_imports_successful = True\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"OK: transformers {}\".format(transformers.__version__))\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        print(\"OK: BitsAndBytesConfig available\")\n",
    "    except ImportError:\n",
    "        print(\"WARNING: BitsAndBytesConfig not available\")\n",
    "        all_imports_successful = False\n",
    "except ImportError as e:\n",
    "    print(\"ERROR: Could not import transformers: {}\".format(str(e)))\n",
    "    all_imports_successful = False\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"OK: torch {}\".format(torch.__version__))\n",
    "except ImportError as e:\n",
    "    print(\"ERROR: Could not import torch: {}\".format(str(e)))\n",
    "    all_imports_successful = False\n",
    "\n",
    "try:\n",
    "    import peft\n",
    "    print(\"OK: peft {}\".format(peft.__version__))\n",
    "except ImportError as e:\n",
    "    print(\"ERROR: Could not import peft: {}\".format(str(e)))\n",
    "    all_imports_successful = False\n",
    "\n",
    "try:\n",
    "    import bitsandbytes\n",
    "    print(\"OK: bitsandbytes {}\".format(bitsandbytes.__version__))\n",
    "    from packaging import version\n",
    "    if version.parse(bitsandbytes.__version__) >= version.parse(\"0.39.0\"):\n",
    "        print(\"    Version OK for 8-bit quantization\")\n",
    "    else:\n",
    "        print(\"    WARNING: Version {} may have compatibility issues\".format(\n",
    "            bitsandbytes.__version__))\n",
    "except ImportError as e:\n",
    "    print(\"WARNING: Could not import bitsandbytes: {}\".format(str(e)))\n",
    "    print(\"    8-bit quantization may not work, but other strategies will\")\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "    print(\"OK: psutil installed\")\n",
    "except ImportError as e:\n",
    "    print(\"WARNING: Could not import psutil: {}\".format(str(e)))\n",
    "\n",
    "try:\n",
    "    import safetensors\n",
    "    print(\"OK: safetensors available\")\n",
    "except ImportError as e:\n",
    "    print(\"WARNING: Could not import safetensors: {}\".format(str(e)))\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "if all_imports_successful:\n",
    "    print(\"ALL PACKAGES INSTALLED AND VERIFIED\")\n",
    "    print(\"Ready to proceed with model optimization\")\n",
    "else:\n",
    "    print(\"PACKAGES INSTALLED WITH WARNINGS\")\n",
    "    print(\"Core functionality available. Some features may have limitations.\")\n",
    "print(\"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T13:41:09.088830Z",
     "iopub.status.busy": "2026-02-05T13:41:09.088215Z",
     "iopub.status.idle": "2026-02-05T13:41:10.464819Z",
     "shell.execute_reply": "2026-02-05T13:41:10.464023Z",
     "shell.execute_reply.started": "2026-02-05T13:41:09.088801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import snapshot_download, hf_hub_download\n",
    "import torch\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import snapshot_download, hf_hub_download\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T13:41:10.466354Z",
     "iopub.status.busy": "2026-02-05T13:41:10.465742Z",
     "iopub.status.idle": "2026-02-05T13:41:22.140899Z",
     "shell.execute_reply": "2026-02-05T13:41:22.140070Z",
     "shell.execute_reply.started": "2026-02-05T13:41:10.466326Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING RAG SYSTEM FROM HUGGING FACE\n",
      "================================================================================\n",
      "\n",
      "[1/5] Configuration Setup\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Device: cuda\n",
      "  GPU: Tesla T4\n",
      "  GPU Memory: 15.64 GB\n",
      "✓ Context Window: 1024 tokens\n",
      "✓ Available for Documents: 823 tokens\n",
      "\n",
      "[2/5] Loading Atlas Model & Tokenizer\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8530ef328ad846c0b725180b63bd3e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Downloaded model to: /root/.cache/huggingface/hub/models--Sally004--Atlas2B_AlgerianDialect_SmokingData/snapshots/f1d49f406b4d684b58910bacffcdfea632b50947\n",
      "  ✓ Fixed tokenizer config\n",
      "✓ Atlas tokenizer loaded\n",
      "✓ Atlas model loaded\n",
      "  Parameters: 2.61B\n",
      "  Memory: ~5.23 GB (FP16)\n",
      "\n",
      "[3/5] Loading Embedding Model\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Embedding model loaded: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "  Embedding dimension: 384\n",
      "\n",
      "[4/5] Downloading Retrieval Files from HF\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Downloaded FAISS index\n",
      "✓ Downloaded chunks\n",
      "✓ Downloaded vectors\n",
      "\n",
      "[5/5] Loading Retrieval Components\n",
      "--------------------------------------------------------------------------------\n",
      "✓ FAISS index loaded: 6142 vectors\n",
      "✓ Chunks loaded: 6142 chunks\n",
      "✓ Metadata loaded\n",
      "  Total chunks: 6142\n",
      "  Average chunk tokens: 586.3637251709541\n",
      "\n",
      "================================================================================\n",
      "DEFINING HELPER FUNCTIONS\n",
      "================================================================================\n",
      "✓ Helper functions defined:\n",
      "  - count_tokens()\n",
      "  - count_tokens_exact()\n",
      "  - embed_query()\n",
      "  - retrieve_documents()\n",
      "\n",
      "================================================================================\n",
      "RAG SYSTEM LOADED SUCCESSFULLY FROM HUGGING FACE!\n",
      "================================================================================\n",
      "\n",
      "Components Ready:\n",
      "  ✓ Atlas Model: 2.61B parameters\n",
      "  ✓ Atlas Tokenizer: Loaded\n",
      "  ✓ Embedding Model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "  ✓ FAISS Index: 6142 vectors\n",
      "  ✓ Document Chunks: 6142\n",
      "  ✓ Device: cuda\n",
      "\n",
      "You can now run the interactive RAG cell!\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOAD FROM HUGGING FACE - ALL NECESSARY VARIABLES & COMPONENTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LOADING RAG SYSTEM FROM HUGGING FACE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: CONFIGURATION\n",
    "# =============================================================================\n",
    "print(\"\\n[1/5] Configuration Setup\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Context & Token Configuration\n",
    "CONTEXT_WINDOW = 1024           # Atlas model context window\n",
    "PROMPT_TOKENS = 186             # System prompt size\n",
    "USER_INPUT_TOKENS = 15          # Average query tokens\n",
    "AVAILABLE_FOR_DOCS = CONTEXT_WINDOW - PROMPT_TOKENS - USER_INPUT_TOKENS\n",
    "\n",
    "# Chunking Configuration (for reference, chunks already created)\n",
    "CHUNK_SIZE_TOKENS = 680\n",
    "CHUNK_OVERLAP_TOKENS = 50\n",
    "MAX_CHUNKS_PER_DOC = 200\n",
    "MAX_CHUNKS_FOR_CONTEXT = 3\n",
    "MAX_TOKENS_PER_RETRIEVAL = AVAILABLE_FOR_DOCS\n",
    "BATCH_SIZE = 32\n",
    "MAX_CONTEXT_LENGTH = 1024\n",
    "# Device Configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"✓ Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"✓ Context Window: {CONTEXT_WINDOW} tokens\")\n",
    "print(f\"✓ Available for Documents: {AVAILABLE_FOR_DOCS} tokens\")\n",
    "\n",
    "# Data Path\n",
    "DATA_PATH_NEW = \"/kaggle/input/rag-data3/Rag_daridja_data_merged_cleaned_algerian.json\"\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: LOAD ATLAS MODEL & TOKENIZER\n",
    "# =============================================================================\n",
    "print(\"\\n[2/5] Loading Atlas Model & Tokenizer\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Download model\n",
    "ATLAS_MERGED_PATH = snapshot_download(\n",
    "    repo_id=\"Sally004/Atlas2B_AlgerianDialect_SmokingData\"\n",
    ")\n",
    "print(f\"✓ Downloaded model to: {ATLAS_MERGED_PATH}\")\n",
    "\n",
    "# Fix tokenizer_config.json\n",
    "config_path = Path(ATLAS_MERGED_PATH) / \"tokenizer_config.json\"\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Remove problematic tokenizer_class field\n",
    "    if 'tokenizer_class' in config:\n",
    "        print(f\"  Fixing tokenizer_class: {config['tokenizer_class']}\")\n",
    "        del config['tokenizer_class']\n",
    "    \n",
    "    # Fix extra_special_tokens if it's a list\n",
    "    if 'extra_special_tokens' in config and isinstance(config['extra_special_tokens'], list):\n",
    "        config['extra_special_tokens'] = {}\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(\"  ✓ Fixed tokenizer config\")\n",
    "\n",
    "# Load tokenizer\n",
    "atlas_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    str(ATLAS_MERGED_PATH),\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"✓ Atlas tokenizer loaded\")\n",
    "\n",
    "# Load model\n",
    "atlas_model = AutoModelForCausalLM.from_pretrained(\n",
    "    str(ATLAS_MERGED_PATH),\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "param_count = sum(p.numel() for p in atlas_model.parameters())\n",
    "print(f\"✓ Atlas model loaded\")\n",
    "print(f\"  Parameters: {param_count / 1e9:.2f}B\")\n",
    "print(f\"  Memory: ~{param_count * 2 / 1e9:.2f} GB (FP16)\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: LOAD EMBEDDING MODEL\n",
    "# =============================================================================\n",
    "print(\"\\n[3/5] Loading Embedding Model\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "embedding_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "embedding_dim = embedding_model.get_sentence_embedding_dimension()\n",
    "print(f\"✓ Embedding model loaded: {EMBED_MODEL_NAME}\")\n",
    "print(f\"  Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: DOWNLOAD RETRIEVAL FILES FROM HUGGING FACE\n",
    "# =============================================================================\n",
    "print(\"\\n[4/5] Downloading Retrieval Files from HF\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "REPO_ID = \"Sally004/Sai_Dataset_NLP\"\n",
    "\n",
    "INDEX_PATH = hf_hub_download(\n",
    "    repo_id=REPO_ID,\n",
    "    filename=\"retrieval/index/vector_index_token_based.faiss\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "print(f\"✓ Downloaded FAISS index\")\n",
    "\n",
    "CHUNKS_PATH = hf_hub_download(\n",
    "    repo_id=REPO_ID,\n",
    "    filename=\"retrieval/chunks/chunks_token_based.pkl\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "print(f\"✓ Downloaded chunks\")\n",
    "\n",
    "VECTORS_PATH = hf_hub_download(\n",
    "    repo_id=REPO_ID,\n",
    "    filename=\"retrieval/vectors/embeddings_token_based.npy\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "print(f\"✓ Downloaded vectors\")\n",
    "\n",
    "EMBEDDING_INFO_PATH = hf_hub_download(\n",
    "    repo_id=REPO_ID,\n",
    "    filename=\"metadata/embedding_info_token.json\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "CHUNK_STATS_PATH = hf_hub_download(\n",
    "    repo_id=REPO_ID,\n",
    "    filename=\"metadata/chunk_stats_token.json\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: LOAD RETRIEVAL COMPONENTS\n",
    "# =============================================================================\n",
    "print(\"\\n[5/5] Loading Retrieval Components\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Load FAISS index\n",
    "index = faiss.read_index(INDEX_PATH)\n",
    "print(f\"✓ FAISS index loaded: {index.ntotal} vectors\")\n",
    "\n",
    "# Load chunks\n",
    "with open(CHUNKS_PATH, \"rb\") as handle:\n",
    "    chunked_documents = pickle.load(handle)\n",
    "print(f\"✓ Chunks loaded: {len(chunked_documents)} chunks\")\n",
    "\n",
    "# Load metadata\n",
    "with open(EMBEDDING_INFO_PATH, \"r\", encoding=\"utf-8\") as handle:\n",
    "    emb_info = json.load(handle)\n",
    "\n",
    "with open(CHUNK_STATS_PATH, \"r\", encoding=\"utf-8\") as handle:\n",
    "    chunk_stats = json.load(handle)\n",
    "\n",
    "print(f\"✓ Metadata loaded\")\n",
    "print(f\"  Total chunks: {chunk_stats.get('total_chunks', len(chunked_documents))}\")\n",
    "print(f\"  Average chunk tokens: {chunk_stats.get('avg_chunk_tokens', 'N/A')}\")\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DEFINING HELPER FUNCTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens using Atlas tokenizer (fallback to heuristic).\"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    try:\n",
    "        return len(atlas_tokenizer.encode(text, add_special_tokens=False))\n",
    "    except Exception:\n",
    "        return max(1, int(len(text.split()) / 0.75))\n",
    "\n",
    "def count_tokens_exact(text, model_type=\"atlas\"):\n",
    "    \"\"\"\n",
    "    Count tokens exactly using the model's tokenizer.\n",
    "    Ensures consistency between chunking and generation.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        if atlas_tokenizer:\n",
    "            return len(atlas_tokenizer.encode(text))\n",
    "        \n",
    "        # Fallback to tiktoken if available\n",
    "        import tiktoken\n",
    "        try:\n",
    "            encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "            return len(encoder.encode(text))\n",
    "        except:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Final fallback: estimate (words / 0.75 for Arabic)\n",
    "    words = len(text.split())\n",
    "    return int(words / 0.75)\n",
    "\n",
    "def embed_query(text):\n",
    "    \"\"\"Embed query into normalized vector space.\"\"\"\n",
    "    try:\n",
    "        vec = embedding_model.encode(text, convert_to_numpy=True)\n",
    "        return (vec / np.linalg.norm(vec)).astype(\"float32\")\n",
    "    except Exception as exc:\n",
    "        print(f\"✗ Error embedding query: {exc}\")\n",
    "        return None\n",
    "\n",
    "def retrieve_documents(question, top_k=5, max_tokens=None):\n",
    "    \"\"\"Retrieve up to top_k chunks while respecting a token budget.\"\"\"\n",
    "    if max_tokens is None:\n",
    "        max_tokens = MAX_TOKENS_PER_RETRIEVAL\n",
    "    top_k = min(top_k, MAX_CHUNKS_FOR_CONTEXT)\n",
    "    query_vec = embed_query(question)\n",
    "    if query_vec is None:\n",
    "        return []\n",
    "    try:\n",
    "        scores, indices = index.search(np.array([query_vec]), top_k * 2)\n",
    "    except Exception as exc:\n",
    "        print(f\"✗ FAISS search error: {exc}\")\n",
    "        return []\n",
    "    results = []\n",
    "    total_tokens = 0\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        if idx < 0 or idx >= len(chunked_documents):\n",
    "            continue\n",
    "        chunk = chunked_documents[idx]\n",
    "        tokens = chunk[\"metadata\"].get(\"token_count\", count_tokens(chunk[\"text\"]))\n",
    "        if results and total_tokens + tokens > max_tokens:\n",
    "            break\n",
    "        results.append({\n",
    "            \"text\": chunk[\"text\"],\n",
    "            \"metadata\": chunk[\"metadata\"],\n",
    "            \"score\": float(score),\n",
    "            \"tokens\": tokens\n",
    "        })\n",
    "        total_tokens += tokens\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "    return sorted(results, key=lambda item: item[\"score\"], reverse=True)\n",
    "\n",
    "print(\"✓ Helper functions defined:\")\n",
    "print(\"  - count_tokens()\")\n",
    "print(\"  - count_tokens_exact()\")\n",
    "print(\"  - embed_query()\")\n",
    "print(\"  - retrieve_documents()\")\n",
    "\n",
    "# Mobile configuration (not used in this setup)\n",
    "USE_MOBILE_FOR_INFERENCE = False\n",
    "ATLAS_MOBILE_PATH = Path(\"/kaggle/input/atlas-mobile/atlas-2B-merged-darija-Q4_K_M.gguf\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RAG SYSTEM LOADED SUCCESSFULLY FROM HUGGING FACE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nComponents Ready:\")\n",
    "print(f\"  ✓ Atlas Model: {param_count / 1e9:.2f}B parameters\")\n",
    "print(f\"  ✓ Atlas Tokenizer: Loaded\")\n",
    "print(f\"  ✓ Embedding Model: {EMBED_MODEL_NAME}\")\n",
    "print(f\"  ✓ FAISS Index: {index.ntotal} vectors\")\n",
    "print(f\"  ✓ Document Chunks: {len(chunked_documents)}\")\n",
    "print(f\"  ✓ Device: {DEVICE}\")\n",
    "print(\"\\nYou can now run the interactive RAG cell!\")\n",
    "print(\"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: Load fine-tuned Atlas model for inference (Support both models)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4/4: LOAD ATLAS MODEL FOR INFERENCE\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "model_type=\"atlas\"\n",
    "print(\"✓ Embedding model loaded\")\n",
    "\n",
    "try:\n",
    "        print(\"\\nLoading model (this may take 1-2 minutes)...\")\n",
    "        param_count = sum(p.numel() for p in atlas_model.parameters())\n",
    "        print(f\"\\n✓ Atlas model loaded successfully\")\n",
    "        print(f\"  Parameters: {param_count / 1e9:.2f}B\")\n",
    "        print(f\"  Memory: ~{param_count * 2 / 1e9:.2f} GB (FP16)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Atlas model: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "def count_tokens_exact(text, model_type=\"atlas\"):\n",
    "    \"\"\"\n",
    "    Count tokens exactly using the model's tokenizer or tiktoken.\n",
    "    Ensures consistency between chunking and generation.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        if atlas_tokenizer:\n",
    "            # For transformers models, use their tokenizer\n",
    "            return len(atlas_tokenizer.encode(text))\n",
    "        \n",
    "        # Fallback to tiktoken if available\n",
    "        import tiktoken\n",
    "        try:\n",
    "            encoder = tiktoken.get_encoding(\"cl100k_base\")  # GPT-3.5/4 tokenizer\n",
    "            return len(encoder.encode(text))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Final fallback: estimate (words / 0.75 for Arabic)\n",
    "    words = len(text.split())\n",
    "    return int(words / 0.75)\n",
    "\n",
    "\n",
    "def build_rag_prompt_with_token_budget(system_prompt, context, question, max_response_tokens):\n",
    "    \"\"\"\n",
    "    Build RAG prompt while ensuring total tokens < CONTEXT_WINDOW.\n",
    "    Returns prompt and actual tokens used.\n",
    "    \"\"\"\n",
    "    # Base tokens for structure\n",
    "    structure_tokens = count_tokens_exact(\"system: \\nuser: السياق:\\n\\nالسؤال:\\n\\nassistant: \")\n",
    "    \n",
    "    # Calculate available tokens for content\n",
    "    total_available = CONTEXT_WINDOW - max_response_tokens - structure_tokens\n",
    "    \n",
    "    # First, count system prompt tokens\n",
    "    system_tokens = count_tokens_exact(system_prompt)\n",
    "    \n",
    "    # Then count question tokens\n",
    "    question_tokens = count_tokens_exact(question)\n",
    "    \n",
    "    # Calculate available for context\n",
    "    available_for_context = total_available - system_tokens - question_tokens\n",
    "    \n",
    "    if available_for_context <= 0:\n",
    "        # Not enough tokens even without context\n",
    "        available_for_context = 50  # Minimum context\n",
    "    \n",
    "    # Truncate context to fit\n",
    "    context_tokens = count_tokens_exact(context)\n",
    "    if context_tokens > available_for_context:\n",
    "        # Need to truncate context\n",
    "        # Estimate characters per token\n",
    "        if context_tokens > 0:\n",
    "            chars_per_token = len(context) / context_tokens\n",
    "            max_chars = int(available_for_context * chars_per_token * 0.9)  # 90% safety\n",
    "            # Try to cut at sentence boundary\n",
    "            if max_chars < len(context):\n",
    "                # Find last space or punctuation\n",
    "                cut_point = max_chars\n",
    "                for i in range(max_chars - 1, max(0, max_chars - 100), -1):\n",
    "                    if context[i] in ['.', '؟', '!', '\\n', ' ']:\n",
    "                        cut_point = i + 1\n",
    "                        break\n",
    "                context = context[:cut_point] + \"...\"\n",
    "                context_tokens = count_tokens_exact(context)\n",
    "    \n",
    "    # Build final prompt\n",
    "    prompt = (\n",
    "        f\"system: {system_prompt}\\n\"\n",
    "        f\"user: السياق:\\n{context}\\n\\n\"\n",
    "        f\"السؤال:\\n{question}\\n\"\n",
    "        f\"assistant: \"\n",
    "    )\n",
    "    \n",
    "    # Calculate total tokens\n",
    "    total_tokens = (\n",
    "        structure_tokens + \n",
    "        system_tokens + \n",
    "        context_tokens + \n",
    "        question_tokens\n",
    "    )\n",
    "    \n",
    "    return prompt, total_tokens, context_tokens\n",
    "\n",
    "\n",
    "def interpret_cosine_similarity(score):\n",
    "    \"\"\"\n",
    "    Interpret cosine similarity score with semantic meaning.\n",
    "    \n",
    "    Cosine similarity ranges from 0 to 1:\n",
    "    1.0 = identical meaning (same vector direction)\n",
    "    0.0 = completely unrelated (orthogonal vectors)\n",
    "    \n",
    "    Returns:\n",
    "        Interpretation dict with category, angle, and confidence level\n",
    "    \"\"\"\n",
    "    # Calculate angle in degrees for better understanding\n",
    "    angle = np.degrees(np.arccos(min(max(score, 0.0), 1.0)))\n",
    "    \n",
    "    if score >= 0.65:\n",
    "        category = \"VERY STRONG MATCH\"\n",
    "        confidence = \"Very High\"\n",
    "        angle_desc = f\"{angle:.1f}° (almost identical direction)\"\n",
    "        action = \"RAG with high confidence\"\n",
    "    elif score >= 0.5:  # NEW THRESHOLD: 0.78\n",
    "        category = \"STRONG MATCH\"\n",
    "        confidence = \"High\"\n",
    "        angle_desc = f\"{angle:.1f}° (very close)\"\n",
    "        action = \"RAG recommended\"\n",
    "    elif score >= 0.45:\n",
    "        category = \"GOOD MATCH\"\n",
    "        confidence = \"Medium-High\"\n",
    "        angle_desc = f\"{angle:.1f}° (good similarity)\"\n",
    "        action = \"RAG suitable\"\n",
    "    elif score >= 0.35:\n",
    "        category = \"MODERATE MATCH\"\n",
    "        confidence = \"Medium\"\n",
    "        angle_desc = f\"{angle:.1f}° (somewhat related)\"\n",
    "        action = \"Consider RAG\"\n",
    "    elif score >= 0.3:\n",
    "        category = \"WEAK MATCH\"\n",
    "        confidence = \"Low\"\n",
    "        angle_desc = f\"{angle:.1f}° (weak relation)\"\n",
    "        action = \"Model knowledge preferred\"\n",
    "    elif score >= 0.25:\n",
    "        category = \"VERY WEAK MATCH\"\n",
    "        confidence = \"Very Low\"\n",
    "        angle_desc = f\"{angle:.1f}° (barely related)\"\n",
    "        action = \"Model knowledge recommended\"\n",
    "    else:\n",
    "        category = \"POOR MATCH\"\n",
    "        confidence = \"None\"\n",
    "        angle_desc = f\"{angle:.1f}° (unrelated)\"\n",
    "        action = \"Use model knowledge only\"\n",
    "    \n",
    "    return {\n",
    "        \"category\": category,\n",
    "        \"confidence_level\": confidence,\n",
    "        \"angle_degrees\": round(angle, 1),\n",
    "        \"angle_description\": angle_desc,\n",
    "        \"recommended_action\": action,\n",
    "        \"score\": score\n",
    "    }\n",
    "\n",
    "\n",
    "def classify_intent_hybrid(question):\n",
    "    \"\"\"\n",
    "    Hybrid semantic intent classifier: rule-based filter + LLM fallback.\n",
    "    Detects greetings, insults, off-topic before RAG retrieval.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (intent_label, canned_response or None)\n",
    "    \"\"\"\n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    # Fast rule-based filter for common greetings (90% coverage, 0ms latency)\n",
    "    greeting_keywords = [\n",
    "        \"سلام\", \"السلام\", \"مرحبا\", \"صباح\", \"مساء\", \n",
    "        \"شكرا\", \"شكراً\", \"أهلا\", \"اهلا\"\n",
    "        \"واش راك\",      # How are you?\n",
    "    \"واش حالك\",     # How are you?\n",
    "    \"واش كيفك\",     # How are you?\n",
    "    \"كيف راك\",      # How are you?\n",
    "    \"كيفاش راك\",    # How are you?\n",
    "    \"واش الحالة\",   # What's up?\n",
    "    \"كيف حالك\",     # How are you?\n",
    "    \"كيف الحال\",  \n",
    "    ]\n",
    "    \n",
    "    words = question_lower.split()\n",
    "    if len(words) <= 4:  # Short messages only\n",
    "        if any(kw in question_lower for kw in greeting_keywords):\n",
    "            # Verify it's not a smoking question with greeting words\n",
    "                return \"تحية\", generate_special_response(\"تحية\", question)\n",
    "    \n",
    "    # For everything else, use LLM few-shot classifier\n",
    "    few_shot_examples = [\n",
    "    # GREETINGS (Adding slang)\n",
    "    (\"واش الحالة\", \"تحية\"),\n",
    "    (\"صحة خويا\", \"تحية\"),\n",
    "    (\"واش يا البوت\", \"تحية\"),\n",
    "    (\"واش راك داير فيها\", \"تحية\"),\n",
    "    \n",
    "    # SMOKING (Adding frustration/emotions)\n",
    "    (\"كرهت حياتي ياخو\", \"سؤال تدخين\"), # Emotional distress is usually about smoking here\n",
    "    (\"غلبتني السيجارة\", \"سؤال تدخين\"),\n",
    "    (\"راني فشلان وتعبان\", \"سؤال تدخين\"),\n",
    "    (\"حاب نتهنى من هاد السم\", \"سؤال تدخين\"),\n",
    "    \n",
    "    # ACTUAL INSULTS (Be very specific)\n",
    "    (\"أنت حمار\", \"سب\"),\n",
    "    (\"تفو عليك\", \"سب\"),\n",
    "    (\"يا ولد الحرام\", \"سب\"),\n",
    "    \n",
    "    # ACTUAL OFF-TOPIC\n",
    "    (\"كيفاش نطيب اللحم؟\", \"غير ذي صلة\"),\n",
    "    (\"شكون ربح الماتش؟\", \"غير ذي صلة\"),\n",
    "    (\"واش رايك في ميسي؟\", \"غير ذي صلة\")\n",
    "]\n",
    "    \n",
    "    examples_text = \"\\n\".join([f\"الرسالة: {q}\\nالتصنيف: {c}\" for q, c in few_shot_examples])\n",
    "    \n",
    "    classifier_prompt = f\"\"\"تصنف الرسالة في واحد من: سؤال تدخين / تحية / سب / غير ذي صلة\n",
    "\n",
    "أمثلة:\n",
    "{examples_text}\n",
    "\n",
    "الرسالة: {question}\n",
    "التصنيف:\"\"\"\n",
    "    \n",
    "    try:\n",
    "            inputs = atlas_tokenizer(\n",
    "                    classifier_prompt,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                ).to(DEVICE)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                    outputs = atlas_model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=15,\n",
    "                        temperature=0.1,\n",
    "                        top_p=0.5,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=atlas_tokenizer.eos_token_id\n",
    "                    )\n",
    "            \n",
    "            intent_raw = atlas_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            if \"التصنيف:\" in intent_raw:\n",
    "                intent_raw = intent_raw.split(\"التصنيف:\")[-1].strip()\n",
    "        \n",
    "            # Count label occurrences for robust classification\n",
    "            scores = {\n",
    "                \"تحية\": intent_raw.count(\"تحية\"),\n",
    "                \"سب\": intent_raw.count(\"سب\"),\n",
    "                \"غير ذي صلة\": intent_raw.count(\"غير ذي صلة\"),\n",
    "                \"سؤال تدخين\": intent_raw.count(\"سؤال تدخين\")\n",
    "            }\n",
    "            \n",
    "            top_intent = max(scores, key=scores.get)\n",
    "            \n",
    "            if top_intent == \"تحية\" or scores[\"تحية\"] > 0:\n",
    "                return \"greeting\", generate_special_response(top_intent, question)\n",
    "            elif top_intent == \"سب\" or scores[\"سب\"] > 0:\n",
    "                return \"insult\", \"معليش، نحترم الجميع هنا. عندك سؤال حول التدخين؟\"\n",
    "            elif top_intent == \"غير ذي صلة\" or scores[\"غير ذي صلة\"] > 0:\n",
    "                return \"off_topic\", \"خاطيني، أنا نجاوب غير على أسئلة التدخين والإقلاع عنه.\"\n",
    "            \n",
    "            return \"smoking\", None\n",
    "        \n",
    "    except Exception as e:\n",
    "            print(f\"Intent classification error: {e}\")\n",
    "            return \"smoking\", None\n",
    "\n",
    "\n",
    "def detect_query_type_by_keywords(question):\n",
    "    \"\"\"\n",
    "    Detect query type using keyword matching for greetings only.\n",
    "    We no longer use keyword detection for offensive language.\n",
    "    \"\"\"\n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    # Greeting patterns in Algerian Arabic/Darija only\n",
    "    greeting_patterns = [\n",
    "        \"السلام\", \"سلام\", \"مرحبا\", \"أهلا\", \"صباح\", \"مساء\", \"مساء الخير\", \"صباح الخير\",\n",
    "        \"اهلا\", \"مرحبا بيك\", \"مرحبا بك\", \"كيف حالك\", \"كيف الحال\", \"واش كي\",\n",
    "         \"مْرحبا\", \"صباح النور\", \"مساء النور\", \"كي راك\", \"واش راك\",\n",
    "        \"واش اخبارك\", \"واش الأوضاع\", \"واش أحوالك\", \"عليكم السلام\", \"وعليكم السلام\",\n",
    "        \"يا مرحبا\", \"سلامو عليكوم\"\n",
    "    ]\n",
    "    \n",
    "    # Check for greeting keywords\n",
    "    greeting_score = 0\n",
    "    for pattern in greeting_patterns:\n",
    "        if pattern in question_lower:\n",
    "            greeting_score += 1\n",
    "    \n",
    "    # Determine query type based on scores\n",
    "    if greeting_score > 0:\n",
    "        return {\n",
    "            \"type\": \"greeting\",\n",
    "            \"detection_method\": \"keyword_matching\",\n",
    "            \"confidence\": min(greeting_score / 3, 1.0),  # Scale to 0-1\n",
    "            \"reason\": f\"Detected {greeting_score} greeting keyword(s)\"\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"type\": \"normal\",\n",
    "            \"detection_method\": \"none\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"reason\": \"No special keywords detected\"\n",
    "        }\n",
    "\n",
    "\n",
    "def detect_special_query_type_from_documents(retrieved_documents, question):\n",
    "    \"\"\"\n",
    "    Detect special query types from retrieved documents AND keyword matching.\n",
    "    This combines both approaches for better accuracy.\n",
    "    \"\"\"\n",
    "    if not retrieved_documents:\n",
    "        return {\n",
    "            \"type\": \"normal\",\n",
    "            \"detection_method\": \"none\",\n",
    "            \"confidence\": 0.0,\n",
    "            \"reason\": \"No documents retrieved\"\n",
    "        }\n",
    "    \n",
    "    # First, check if any retrieved document has special titles\n",
    "    special_titles = {\n",
    "        \"ترحيب عام واسئلة اجتماعية\": \"تحية\",\n",
    "        \"كلام قبيح أو سب\": \"سب\"\n",
    "    }\n",
    "    GREET_THRESHOLD = 0.85\n",
    "    \n",
    "    best_score = retrieved_documents[0][\"score\"] if retrieved_documents else 0\n",
    "    best_title = retrieved_documents[0][\"metadata\"].get(\"title\", \"N/A\") if retrieved_documents else \"\"\n",
    "    \n",
    "    # Check if the best document has a special title\n",
    "    if best_title in special_titles:\n",
    "        if best_title == \"ترحيب عام واسئلة اجتماعية\" and best_score < GREET_THRESHOLD:\n",
    "            pass\n",
    "        else:\n",
    "            # For offensive language, only trust if score >= 0.7\n",
    "            if special_titles[best_title] == \"offensive\" and best_score < 0.7:\n",
    "                # Offensive document but low similarity, might be false positive\n",
    "                return {\n",
    "                    \"type\": \"normal\",\n",
    "                    \"detection_method\": \"document_title_but_low_score\",\n",
    "                    \"confidence\": best_score,\n",
    "                    \"reason\": f\"Offensive document found but low similarity ({best_score:.4f} < 0.7)\"\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                \"type\": special_titles[best_title],\n",
    "                \"detection_method\": \"document_title\",\n",
    "                \"confidence\": best_score,\n",
    "                \"reason\": f\"Document title indicates {special_titles[best_title]} type\"\n",
    "            }\n",
    "    \n",
    "    # If not found in titles, check all retrieved documents for offensive\n",
    "    offensive_detected = False\n",
    "    offensive_score = 0\n",
    "    for doc in retrieved_documents[:3]:  # Check top 3\n",
    "        title = doc[\"metadata\"].get(\"title\", \"\")\n",
    "        if title == \"كلام قبيح أو سب\":\n",
    "            offensive_detected = True\n",
    "            offensive_score = doc[\"score\"]\n",
    "            if doc[\"score\"] >= 0.7:\n",
    "                return {\n",
    "                    \"type\": \"offensive\",\n",
    "                    \"detection_method\": \"document_title_in_top_3\",\n",
    "                    \"confidence\": doc[\"score\"],\n",
    "                    \"reason\": f\"Offensive document found with high similarity ({doc['score']:.4f} ≥ 0.7)\"\n",
    "                }\n",
    "    \n",
    "    # If offensive detected but score too low, note it\n",
    "    if offensive_detected:\n",
    "        return {\n",
    "            \"type\": \"normal\",\n",
    "            \"detection_method\": \"document_title_but_low_score\",\n",
    "            \"confidence\": offensive_score,\n",
    "            \"reason\": f\"Offensive document found but low similarity ({offensive_score:.4f} < 0.7)\"\n",
    "        }\n",
    "    \n",
    "    # If still not found, use keyword matching for greetings only\n",
    "    keyword_result = detect_query_type_by_keywords(question)\n",
    "    \n",
    "    # Only trust keyword detection for greetings if confidence is high enough\n",
    "    if keyword_result[\"type\"] == \"greeting\" and keyword_result[\"confidence\"] > 0.3:\n",
    "        return keyword_result\n",
    "    \n",
    "    return {\n",
    "        \"type\": \"normal\",\n",
    "        \"detection_method\": \"none\",\n",
    "        \"confidence\": 0.0,\n",
    "        \"reason\": \"Normal smoking-related query\"\n",
    "    }\n",
    "\n",
    "    \n",
    "def embed_query_simple(text):\n",
    "    \"\"\"Embed text using pre-loaded model.\"\"\"\n",
    "    if not text or embedding_model is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        embedding = embedding_model.encode(text, convert_to_numpy=True)\n",
    "        embedding = embedding / np.linalg.norm(embedding)\n",
    "        return embedding.astype(\"float32\")\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error: {e}\")\n",
    "        return None\n",
    "        \n",
    "def select_greeting_by_similarity(user_question):\n",
    "    \"\"\"Select most appropriate greeting based on semantic similarity using anchor phrases.\"\"\"\n",
    "    \n",
    "    # 1. Map 'Anchors' (what user says) to 'Responses' (what bot says)\n",
    "    # This ensures high similarity scores\n",
    "    greeting_map = [\n",
    "        {\"anchors\": [\"السلام عليكم\", \"سلام\"], \"response\": \"وعليكم السلام ورحمة الله! مرحبا بيك، قولي واش هو سؤالك على التدخين؟\"},\n",
    "        {\"anchors\": [\"أهلا\", \"مرحبا\", \"واش راك\"], \"response\": \"أهلا بيك! واش راك؟ كيفاش نقدر نعاونك اليوم في موضوع التدخين؟\"},\n",
    "        {\"anchors\": [\"صباح الخير\", \"كي صبحت\"], \"response\": \"صباح النور والسرور! واش راك؟ كاش ما نقدر نعاونك في موضوع التدخين اليوم؟\"},\n",
    "        {\"anchors\": [\"مساء الخير\", \"كي عشيت\"], \"response\": \"مساء الخير والأنوار! واش أحوالك؟ راني هنا إذا سحقيت كاش نصيحة على التدخين\"}\n",
    "    ]\n",
    "    \n",
    "    # Extract just the first anchor for embedding (or a representative phrase)\n",
    "    anchor_phrases = [m[\"anchors\"][0] for m in greeting_map]\n",
    "    responses = [m[\"response\"] for m in greeting_map]\n",
    "\n",
    "    # 2. Embed user question\n",
    "    user_embedding = embed_query_simple(user_question)\n",
    "    if user_embedding is None:\n",
    "        return responses[1], 0.0 # Default to general 'Ahlan'\n",
    "    \n",
    "    # 3. Embed the Anchors (Note: In production, do this ONCE outside the function to save speed)\n",
    "    greeting_embeddings = []\n",
    "    for anchor in anchor_phrases:\n",
    "        emb = embed_query_simple(anchor)\n",
    "        if emb is not None:\n",
    "            greeting_embeddings.append(emb)\n",
    "        else:\n",
    "            # Fallback to zero vector if embedding fails\n",
    "            greeting_embeddings.append(np.zeros_like(user_embedding))\n",
    "    \n",
    "    # 4. Calculate similarities\n",
    "    greeting_embeddings = np.array(greeting_embeddings)\n",
    "    \n",
    "    # Ensure vectors are normalized for true cosine similarity\n",
    "    # similarity = dot(A, B) / (norm(A) * norm(B))\n",
    "    similarities = np.dot(greeting_embeddings, user_embedding)\n",
    "    \n",
    "    # 5. Find best match\n",
    "    best_idx = np.argmax(similarities)\n",
    "    best_similarity = float(similarities[best_idx])\n",
    "    selected_greeting = responses[best_idx]\n",
    "    \n",
    "    return selected_greeting, best_similarity\n",
    "    \n",
    "def generate_special_response(query_type_info, question):\n",
    "    \"\"\"\n",
    "    Generate response for special query types (greetings, offensive language).\n",
    "    \"\"\"\n",
    "    if isinstance(query_type_info, str):\n",
    "        q_type = query_type_info\n",
    "        if query_type_info == \"تحية\":\n",
    "            selected, similarity = select_greeting_by_similarity(question)\n",
    "            return selected\n",
    "        elif query_type_info == \"سب\":\n",
    "            # Direct refusal for offensive language in Algerian Darija\n",
    "            return \"معليش، ما نقدرش نجاوب على هاد النوع من الكلام. تكلم باحترام ونقدر نعاونك.\"\n",
    "    else:\n",
    "        q_type = query_type_info.get(\"type\")\n",
    "        if query_type_info[\"type\"] == \"تحية\":\n",
    "            # Simple greeting response in Algerian Darija\n",
    "            selected, similarity = select_greeting_by_similarity(question)\n",
    "            return selected\n",
    "        elif query_type_info[\"type\"] == \"سب\" :\n",
    "            # Direct refusal for offensive language in Algerian Darija\n",
    "            return \"معليش، ما نقدرش نجاوب على هاد النوع من الكلام. تكلم باحترام ونقدر نعاونك.\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def generate_answer(question, context=None, max_tokens_for_response=120, use_rag=True, temperature=None):\n",
    "    \"\"\"\n",
    "    Generate answer using fine-tuned Atlas model with anti-hallucination measures.\n",
    "    \n",
    "    Improvements:\n",
    "    - Chain-of-verification prompt (hidden reasoning steps)\n",
    "    - Lower temperature for RAG (0.2 vs 0.35)\n",
    "    - Lower repetition penalty (1.2 vs 1.5)\n",
    "    - Dynamic token allocation (80-150 based on confidence)\n",
    "    \n",
    "    Args:\n",
    "        question: User's question (string)\n",
    "        context: Retrieved context from RAG (string) or None if using model knowledge\n",
    "        max_tokens_for_response: Max tokens to generate\n",
    "        use_rag: Whether to use RAG context or model knowledge\n",
    "        temperature: Specific temperature to use (if None, uses default based on use_rag)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure max_tokens_for_response is reasonable\n",
    "    \n",
    "    if use_rag and context:\n",
    "        # RAG with anti-hallucination prompt\n",
    "        system_prompt = (\n",
    "        \"أنت خبير جزائري مختص في التوعية ضد التدخين. \"\n",
    "        \"تحدث بالدارجة الجزائرية البيضاء (فصيحة تقنياً). \"\n",
    "        \"التزم بالحقائق العلمية فقط. ممنوع الدراما، ممنوع السياسة، وممنوع التحدث عن دول أخرى .جاوب مباشرة تقنياً. ابدأ الجواب بـ 'بناءً على المعلومات المتوفرة\"\n",
    "        \"خاطب المستخدم بصيغة المذكر دائماً إلا إذا ذكر عكس ذلك.\"\n",
    "    )\n",
    "        # Anti-hallucination: Add hidden reasoning steps\n",
    "        prompt = f\"\"\"system: {system_prompt}\n",
    "\n",
    "{context if context else \"ملاحظة: لا يوجد سياق خارجي، جاوب باختصار من القواعد العامة للإقلاع عن التدخين.\"}\n",
    "\n",
    "التعليمات الإجبارية:\n",
    "1. استخرج النصائح من السياق (Context) إذا كان متوفراً.\n",
    "2. إذا كان السؤال بعيداً عن التدخين، قل: \"أنا هنا للمساعدة في الإقلاع عن التدخين فقط\".\n",
    "3. ممنوع نهائياً ذكر أي جمل درامية غير واقعية.\n",
    "4. الجواب يكون في شكل نقاط (Bullet points) ليكون واضحاً.\n",
    "5. لا تزد عن 80 كلمة لتجنب انقطاع النص.\n",
    "\n",
    "user: السؤال: {question}\n",
    "\n",
    "assistant: \"\"\"\n",
    "        \n",
    "        actual_prompt_tokens = count_tokens(prompt)\n",
    "        max_safe = CONTEXT_WINDOW - actual_prompt_tokens - 10\n",
    "        max_tokens_for_response = min(max_tokens_for_response, max_safe)\n",
    "        # Lower temperature for factual responses\n",
    "        if temperature is None:\n",
    "            temperature = 0.2  # Was 0.35\n",
    "        \n",
    "        # Calculate tokens for budget tracking\n",
    "        prompt_tokens = count_tokens_exact(prompt)\n",
    "        \n",
    "    else:\n",
    "        # Model knowledge (no RAG)\n",
    "        system_prompt = (\n",
    "            \"أنت مساعد جزائري مختص في التدخين والإقلاع عنه، تهدر بالدارجة الجزائرية. \"\n",
    "            \"جاوب بإجابات قصيرة ومباشرة وعملية بلا خطبة. ممنوع تمد نصائح طبية من راسك. ممنوع الفلسفة.\"\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"system: {system_prompt}\n",
    "\n",
    "user: {question}\n",
    "\n",
    "assistant: \"\"\"\n",
    "        \n",
    "        if temperature is None:\n",
    "            temperature = 0.4  # Was 0.7\n",
    "        \n",
    "        prompt_tokens = count_tokens_exact(prompt)\n",
    "    \n",
    "    try:\n",
    "        inputs = atlas_tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=MAX_CONTEXT_LENGTH\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "                outputs = atlas_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_tokens_for_response,\n",
    "                    temperature=temperature,\n",
    "                    top_p=0.4,              # Was 0.8\n",
    "                    top_k=40,\n",
    "                    do_sample=True,\n",
    "                    repetition_penalty=1.2,  # Was 1.15\n",
    "                    pad_token_id=atlas_tokenizer.eos_token_id,\n",
    "                    eos_token_id=atlas_tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "                answer = atlas_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                if \"assistant:\" in answer:\n",
    "                    answer = answer.split(\"assistant:\")[-1].strip()\n",
    "        \n",
    "        # Return answer and token usage info\n",
    "        answer_tokens = count_tokens_exact(answer)\n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"total_prompt_tokens\": prompt_tokens,\n",
    "            \"answer_tokens\": answer_tokens,\n",
    "            \"context_tokens_used\": count_tokens_exact(context) if context else 0,\n",
    "            \"total_tokens\": prompt_tokens + answer_tokens\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer: {str(e)}\")\n",
    "        return {\n",
    "            \"answer\": \"معليش، وقعت مشكلة في الجواب. حاول تاني بعد شوية.\",\n",
    "            \"total_prompt_tokens\": 0,\n",
    "            \"answer_tokens\": 0,\n",
    "            \"context_tokens_used\": 0,\n",
    "            \"total_tokens\": 0\n",
    "        }\n",
    "\n",
    "\n",
    "def process_query(question):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with semantic intent classification and anti-hallucination.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Pre-RAG intent classification (greetings/off-topic/insults)\n",
    "    2. Document retrieval (only for smoking questions)\n",
    "    3. Dynamic token allocation (80-150 based on confidence)\n",
    "    4. Anti-hallucination generation\n",
    "    \n",
    "    Updated for token-based retrieval with budget enforcement.\n",
    "    \"\"\"\n",
    "    \n",
    "    # STEP 1: Pre-RAG Intent Classification\n",
    "    intent, canned_response = classify_intent_hybrid(question)\n",
    "    \n",
    "    # If non-smoking intent detected, return canned response immediately\n",
    "    if canned_response:\n",
    "        return {\n",
    "            \"answer\": canned_response,\n",
    "            \"confidence\": 1.0,\n",
    "            \"selected_document\": None,\n",
    "            \"all_documents\": [],\n",
    "            \"context_tokens\": 0,\n",
    "            \"available_for_response\": 0,\n",
    "            \"rag_used\": False,\n",
    "            \"query_type\": intent,\n",
    "            \"reason\": f\"Intent classification: {intent} (handled pre-RAG)\",\n",
    "            \"similarity_interpretation\": None,\n",
    "            \"special_handling\": True,\n",
    "            \"token_usage\": {\n",
    "                \"prompt_tokens\": count_tokens_exact(f\"system: \\nuser: {question}\\nassistant: \"),\n",
    "                \"answer_tokens\": count_tokens_exact(canned_response),\n",
    "                \"total_tokens\": count_tokens_exact(f\"system: \\nuser: {question}\\nassistant: {canned_response}\"),\n",
    "                \"context_window\": CONTEXT_WINDOW\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # STEP 2: Calculate max tokens for context retrieval\n",
    "    system_prompt_tokens = count_tokens_exact(\n",
    "        \"أنت مساعد جزائري مختص في التدخين والإقلاع عنه، تهدر بالدارجة الجزائرية. \"\n",
    "        \"جاوب بإجابات قصيرة ومباشرة وعملية بلا خطبة.\"\n",
    "    )\n",
    "    \n",
    "    question_tokens = count_tokens_exact(question)\n",
    "    structure_tokens = count_tokens_exact(\"system: \\nuser: السياق:\\n\\nالسؤال:\\n\\nassistant: \")\n",
    "    \n",
    "    # Reserve tokens for response \n",
    "    response_tokens_reserved = 0\n",
    "    \n",
    "    # Calculate available for context\n",
    "    max_context_tokens = CONTEXT_WINDOW - (\n",
    "        system_prompt_tokens + \n",
    "        question_tokens + \n",
    "        structure_tokens + \n",
    "        response_tokens_reserved\n",
    "    )\n",
    "    \n",
    "    # Ensure we have at least some context\n",
    "    if max_context_tokens < 50:\n",
    "        max_context_tokens = 50\n",
    "    \n",
    "    # STEP 3: Retrieve documents\n",
    "    retrieved = retrieve_documents(\n",
    "        question, \n",
    "        top_k=5,\n",
    "        max_tokens=max_context_tokens\n",
    "    )\n",
    "    \n",
    "    if not retrieved:\n",
    "        # No documents found, use model knowledge\n",
    "        result = generate_answer(question, context=None, max_tokens_for_response=80, use_rag=False)\n",
    "        return {\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"confidence\": 0.0,\n",
    "            \"selected_document\": None,\n",
    "            \"all_documents\": [],\n",
    "            \"context_tokens\": 0,\n",
    "            \"available_for_response\": AVAILABLE_FOR_DOCS,\n",
    "            \"rag_used\": False,\n",
    "            \"query_type\": \"smoking\",\n",
    "            \"reason\": \"No relevant documents found\",\n",
    "            \"similarity_interpretation\": interpret_cosine_similarity(0.0),\n",
    "            \"token_usage\": {\n",
    "                \"prompt_tokens\": result.get(\"total_prompt_tokens\", 0),\n",
    "                \"answer_tokens\": result.get(\"answer_tokens\", 0),\n",
    "                \"total_tokens\": result.get(\"total_tokens\", 0),\n",
    "                \"context_window\": CONTEXT_WINDOW\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    best_doc = retrieved[0]\n",
    "    best_score = best_doc[\"score\"]\n",
    "    best_title = best_doc[\"metadata\"].get(\"title\", \"N/A\")\n",
    "    context = best_doc[\"text\"]\n",
    "    \n",
    "    # Check for special document-based detection (offensive language)\n",
    "    query_type_info = detect_special_query_type_from_documents(retrieved, question)\n",
    "    \n",
    "    # If it's a special query type, generate appropriate response\n",
    "    if query_type_info[\"type\"] in [\"تحية\", \"سب\"]:\n",
    "        answer = generate_special_response(query_type_info, question)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"confidence\": round(best_score, 4),\n",
    "            \"selected_document\": {\n",
    "                \"rank\": 1,\n",
    "                \"title\": best_title,\n",
    "                \"score\": best_score,\n",
    "                \"tokens\": best_doc.get(\"tokens\", 0)\n",
    "            },\n",
    "            \"all_documents\": [\n",
    "                {\n",
    "                    \"rank\": i + 1,\n",
    "                    \"title\": doc[\"metadata\"].get(\"title\", \"N/A\"),\n",
    "                    \"score\": round(doc[\"score\"], 4),\n",
    "                    \"tokens\": doc.get(\"tokens\", 0)\n",
    "                }\n",
    "                for i, doc in enumerate(retrieved)\n",
    "            ],\n",
    "            \"context_tokens\": 0,\n",
    "            \"available_for_response\": AVAILABLE_FOR_DOCS,\n",
    "            \"rag_used\": False,\n",
    "            \"query_type\": query_type_info[\"type\"],\n",
    "            \"detection_method\": query_type_info[\"detection_method\"],\n",
    "            \"reason\": f\"{query_type_info['reason']} (confidence: {query_type_info['confidence']:.2f})\",\n",
    "            \"similarity_interpretation\": interpret_cosine_similarity(best_score),\n",
    "            \"special_handling\": True,\n",
    "            \"token_usage\": {\n",
    "                \"prompt_tokens\": count_tokens_exact(f\"system: \\nuser: {question}\\nassistant: \"),\n",
    "                \"answer_tokens\": count_tokens_exact(answer),\n",
    "                \"total_tokens\": count_tokens_exact(f\"system: \\nuser: {question}\\nassistant: {answer}\"),\n",
    "                \"context_window\": CONTEXT_WINDOW\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Get interpretation of the similarity score\n",
    "    similarity_interpretation = interpret_cosine_similarity(best_score)\n",
    "    \n",
    "    # STEP 4: Dynamic token allocation based on confidence\n",
    "    CONFIDENCE_THRESHOLD = 0.65\n",
    "    \n",
    "    if best_score >= 0.77:\n",
    "        max_tokens_for_response = 250\n",
    "        temperature = 0.32\n",
    "    elif best_score >= CONFIDENCE_THRESHOLD:\n",
    "        max_tokens_for_response = 180\n",
    "        temperature = 0.4\n",
    "    elif best_score >= 0.45:\n",
    "        max_tokens_for_response = 150\n",
    "        temperature = 0.45\n",
    "        # ← Move HERE: Always concatenate all docs first\n",
    "        context_parts = []\n",
    "        for doc in retrieved:\n",
    "            context_parts.append(f\"[{doc['metadata']['title']}]\\n{doc['text']}\")\n",
    "        context = \"\\n---\\n\".join(context_parts)\n",
    "        \n",
    "        # Calculate context tokens and available for response\n",
    "        context_tokens_used = sum(doc[\"tokens\"] for doc in retrieved)\n",
    "        available_for_response = CONTEXT_WINDOW - (\n",
    "            system_prompt_tokens + \n",
    "            question_tokens + \n",
    "            structure_tokens + \n",
    "            context_tokens_used\n",
    "        )\n",
    "        \n",
    "        if available_for_response < 100:\n",
    "            available_for_response = 100\n",
    "            max_context_tokens = CONTEXT_WINDOW - (\n",
    "                system_prompt_tokens + \n",
    "                question_tokens + \n",
    "                structure_tokens + \n",
    "                available_for_response\n",
    "            )\n",
    "            retrieved = retrieve_documents(question, top_k=5, max_tokens=max_context_tokens)\n",
    "            if retrieved:\n",
    "                best_doc = retrieved[0]\n",
    "                context_parts = []\n",
    "                for doc in retrieved:\n",
    "                    context_parts.append(f\"[{doc['metadata']['title']}]\\n{doc['text']}\")\n",
    "                context = \"\\n---\\n\".join(context_parts)\n",
    "                context_tokens_used = sum(doc[\"tokens\"] for doc in retrieved)\n",
    "        \n",
    "        # Use RAG with medium confidence\n",
    "        result = generate_answer(question, context, max_tokens_for_response=max_tokens_for_response, \n",
    "                               use_rag=True, temperature=temperature)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"confidence\": round(best_score, 4),\n",
    "            \"selected_document\": {\n",
    "                \"rank\": 1,\n",
    "                \"title\": best_title,\n",
    "                \"score\": best_score,\n",
    "                \"tokens\": best_doc.get(\"tokens\", 0)\n",
    "            },\n",
    "            \"all_documents\": [\n",
    "                {\n",
    "                    \"rank\": i + 1,\n",
    "                    \"title\": doc[\"metadata\"].get(\"title\", \"N/A\"),\n",
    "                    \"score\": round(doc[\"score\"], 4),\n",
    "                    \"tokens\": doc.get(\"tokens\", 0)\n",
    "                }\n",
    "                for i, doc in enumerate(retrieved)\n",
    "            ],\n",
    "            \"context_tokens\": context_tokens_used,\n",
    "            \"available_for_response\": max_tokens_for_response,\n",
    "            \"rag_used\": True,\n",
    "            \"query_type\": \"smoking\",\n",
    "            \"reason\": f\"Good match RAG ({best_score:.4f} ≥ 0.70)\",\n",
    "            \"similarity_interpretation\": similarity_interpretation,\n",
    "            \"generation_temperature\": temperature,\n",
    "            \"token_usage\": {\n",
    "                \"prompt_tokens\": result.get(\"total_prompt_tokens\", 0),\n",
    "                \"answer_tokens\": result.get(\"answer_tokens\", 0),\n",
    "                \"context_tokens\": result.get(\"context_tokens_used\", 0),\n",
    "                \"total_tokens\": result.get(\"total_tokens\", 0),\n",
    "                \"context_window\": CONTEXT_WINDOW\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        # Confidence too low, use model knowledge\n",
    "        if best_score >= 0.3:\n",
    "            temperature = 0.45\n",
    "            max_tokens = 80\n",
    "        else:\n",
    "            temperature = 0.5\n",
    "            max_tokens = 70\n",
    "        \n",
    "        result = generate_answer(question, context=None, max_tokens_for_response=max_tokens, \n",
    "                               use_rag=False, temperature=temperature)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"confidence\": round(best_score, 4),\n",
    "            \"selected_document\": {\n",
    "                \"rank\": 1,\n",
    "                \"title\": best_title,\n",
    "                \"score\": best_score,\n",
    "                \"tokens\": best_doc.get(\"tokens\", 0)\n",
    "            },\n",
    "            \"all_documents\": [\n",
    "                {\n",
    "                    \"rank\": i + 1,\n",
    "                    \"title\": doc[\"metadata\"].get(\"title\", \"N/A\"),\n",
    "                    \"score\": round(doc[\"score\"], 4),\n",
    "                    \"tokens\": doc.get(\"tokens\", 0)\n",
    "                }\n",
    "                for i, doc in enumerate(retrieved)\n",
    "            ],\n",
    "            \"context_tokens\": 0,\n",
    "            \"available_for_response\": AVAILABLE_FOR_DOCS,\n",
    "            \"rag_used\": False,\n",
    "            \"query_type\": \"smoking\",\n",
    "            \"reason\": f\"Cosine similarity {best_score:.4f} < {CONFIDENCE_THRESHOLD} (threshold)\",\n",
    "            \"similarity_interpretation\": similarity_interpretation,\n",
    "            \"generation_temperature\": temperature,\n",
    "            \"model_knowledge_tokens\": max_tokens,\n",
    "            \"token_usage\": {\n",
    "                \"prompt_tokens\": result.get(\"total_prompt_tokens\", 0),\n",
    "                \"answer_tokens\": result.get(\"answer_tokens\", 0),\n",
    "                \"total_tokens\": result.get(\"total_tokens\", 0),\n",
    "                \"context_window\": CONTEXT_WINDOW\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # High confidence RAG path (>= 0.78)\n",
    "    context_tokens_used = sum(doc[\"tokens\"] for doc in retrieved)\n",
    "    \n",
    "    # Calculate available for response\n",
    "    available_for_response = CONTEXT_WINDOW - (\n",
    "        system_prompt_tokens + \n",
    "        question_tokens + \n",
    "        structure_tokens + \n",
    "        context_tokens_used\n",
    "    )\n",
    "    \n",
    "    # Ensure we have at least minimum tokens for response\n",
    "    if available_for_response < max_tokens_for_response:\n",
    "        max_tokens_for_response = max(55, min(max_tokens_for_response, available_for_response))\n",
    "        if available_for_response < 55:\n",
    "            # Need to reduce context\n",
    "            available_for_response = 55\n",
    "            max_context_tokens = CONTEXT_WINDOW - (\n",
    "                system_prompt_tokens + \n",
    "                question_tokens + \n",
    "                structure_tokens + \n",
    "                available_for_response\n",
    "            )\n",
    "            retrieved = retrieve_documents(question, top_k=5, max_tokens=max_context_tokens)\n",
    "            if retrieved:\n",
    "                best_doc = retrieved[0]\n",
    "                context_parts = []\n",
    "                for doc in retrieved:\n",
    "                    context_parts.append(f\"[{doc['metadata']['title']}]\\n{doc['text']}\")\n",
    "                context = \"\\n---\\n\".join(context_parts)\n",
    "                context_tokens_used = sum(doc[\"tokens\"] for doc in retrieved)\n",
    "    \n",
    "    result = generate_answer(question, context, max_tokens_for_response=max_tokens_for_response, \n",
    "                           use_rag=True, temperature=temperature)\n",
    "    \n",
    "    return {\n",
    "        \"answer\": result[\"answer\"],\n",
    "        \"confidence\": round(best_score, 4),\n",
    "        \"selected_document\": {\n",
    "            \"rank\": 1,\n",
    "            \"title\": best_title,\n",
    "            \"score\": best_score,\n",
    "            \"tokens\": best_doc.get(\"tokens\", 0)\n",
    "        },\n",
    "        \"all_documents\": [\n",
    "            {\n",
    "                \"rank\": i + 1,\n",
    "                \"title\": doc[\"metadata\"].get(\"title\", \"N/A\"),\n",
    "                \"score\": round(doc[\"score\"], 4),\n",
    "                \"tokens\": doc.get(\"tokens\", 0)\n",
    "            }\n",
    "            for i, doc in enumerate(retrieved)\n",
    "        ],\n",
    "        \"context_tokens\": context_tokens_used,\n",
    "        \"available_for_response\": max_tokens_for_response,\n",
    "        \"rag_used\": True,\n",
    "        \"query_type\": \"smoking\",\n",
    "        \"reason\": f\"High confidence RAG ({best_score:.4f} ≥ {CONFIDENCE_THRESHOLD})\",\n",
    "        \"similarity_interpretation\": similarity_interpretation,\n",
    "        \"generation_temperature\": temperature,\n",
    "        \"token_usage\": {\n",
    "            \"prompt_tokens\": result.get(\"total_prompt_tokens\", 0),\n",
    "            \"answer_tokens\": result.get(\"answer_tokens\", 0),\n",
    "            \"context_tokens\": result.get(\"context_tokens_used\", 0),\n",
    "            \"total_tokens\": result.get(\"total_tokens\", 0),\n",
    "            \"context_window\": CONTEXT_WINDOW\n",
    "        }\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# INTERACTIVE RAG SYSTEM\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RAG SYSTEM READY FOR QUERIES!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nSystem Architecture:\")\n",
    "print(\"   Input: Rag_daridja_data_ar_only.json (token-based chunked)\")\n",
    "print(\"   Embedding: paraphrase-multilingual-MiniLM → 384-dim vectors\")\n",
    "print(\"   Search: FAISS cosine similarity (normalized inner product)\")\n",
    "print(\"   Generation: Fine-tuned Atlas model (using\", model_type + \")\")\n",
    "print(\"\\nKey Improvements:\")\n",
    "print(\"   Pre-RAG Intent Classification: Hybrid rule-based + LLM few-shot\")\n",
    "print(\"   Threshold: 0.65 for RAG utilization (strong match or better)\")\n",
    "print(\"   RAG Answer Tokens: 100-150 tokens for detailed smoking answers\")\n",
    "print(\"   Anti-Hallucination: Chain-of-verification prompts with hidden reasoning\")\n",
    "print(\"   Generation Params: temp=0.2-0.4 (RAG), repetition_penalty=1.2\")\n",
    "print(\"\\nIntent Handling:\")\n",
    "print(\"   Greetings: Detected pre-RAG, clean response (no smoking hallucination)\")\n",
    "print(\"   Off-topic: Polite rejection, no elaboration\")\n",
    "print(\"   Insults: Respectful deflection, redirect to smoking questions\")\n",
    "print(\"   Offensive Detection: Via 'كلام قبيح أو سب' doc with score ≥ 0.7\")\n",
    "print(\"\\nToken Budget Enforcement:\")\n",
    "print(f\"   Context Window: {CONTEXT_WINDOW} tokens (hard limit)\")\n",
    "print(f\"   Exact token counting for system prompt + context + question + response\")\n",
    "print(f\"   Automatic truncation to fit within {CONTEXT_WINDOW} tokens\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "def run_interactive_session():\n",
    "    \"\"\"\n",
    "    Interactive RAG chatbot with complete intent classification and anti-hallucination.\n",
    "    \"\"\"\n",
    "    print(\"Interactive RAG System - Algerian Darija Smoking Cessation Assistant\")\n",
    "    print(\"   Type 'exit' or 'quit' to stop\\n\")\n",
    "    \n",
    "    query_count = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nYour question (Arabic/Darija): \").strip()\n",
    "            \n",
    "            if user_input.lower() in [\"exit\", \"quit\", \"خروج\"]:\n",
    "                print(f\"\\n✓ Session ended. Total queries: {query_count}\")\n",
    "                print(\"Goodbye / بسلامة\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            query_count += 1\n",
    "            print(f\"\\n{'=' * 80}\")\n",
    "            print(f\"QUERY #{query_count}\")\n",
    "            print('=' * 80)\n",
    "            \n",
    "            result = process_query(user_input)\n",
    "            \n",
    "            # Check if special handling (greeting, off-topic, insult)\n",
    "            if result.get(\"special_handling\", False):\n",
    "                print(f\"\\nINTENT CLASSIFICATION (Pre-RAG)\")\n",
    "                print(\"-\" * 80)\n",
    "                print(f\"  Detected Type: {result['query_type'].upper()}\")\n",
    "                print(f\"  Reason: {result['reason']}\")\n",
    "                print(f\"  Retrieval: SKIPPED (handled before RAG)\")\n",
    "            else:\n",
    "                print(\"\\nRETRIEVED DOCUMENTS (Top 5, ranked by cosine similarity)\")\n",
    "                print(\"-\" * 80)\n",
    "                for doc in result[\"all_documents\"]:\n",
    "                    marker = \" ✓ SELECTED\" if doc[\"rank\"] == 1 else \"\"\n",
    "                    print(f\"  [{doc['rank']}] {doc['title']}{marker}\")\n",
    "                    print(f\"      Score: {doc['score']:.4f} | Tokens: {doc['tokens']}\")\n",
    "                \n",
    "                if result[\"selected_document\"]:\n",
    "                    print(f\"\\nCOSINE SIMILARITY ANALYSIS\")\n",
    "                    print(\"-\" * 80)\n",
    "                    print(f\"  Top Document: {result['selected_document']['title']}\")\n",
    "                    print(f\"  Cosine Similarity Score: {result['confidence']:.4f}\")\n",
    "                    \n",
    "                    if \"similarity_interpretation\" in result and result[\"similarity_interpretation\"]:\n",
    "                        interp = result[\"similarity_interpretation\"]\n",
    "                        print(f\"  Category: {interp['category']}\")\n",
    "                        print(f\"  Vector Angle: {interp['angle_description']}\")\n",
    "                        print(f\"  Confidence Level: {interp['confidence_level']}\")\n",
    "                        print(f\"  Recommended Action: {interp['recommended_action']}\")\n",
    "                    \n",
    "                    print(f\"\\nDECISION & GENERATION SETTINGS\")\n",
    "                    print(\"-\" * 80)\n",
    "                    print(f\"  Threshold: 0.65 (STRONG MATCH or better)\")\n",
    "                    print(f\"  Decision: {'RAG (context used)' if result['rag_used'] else 'Model Knowledge (no context)'}\")\n",
    "                    print(f\"  Reason: {result['reason']}\")\n",
    "                    \n",
    "                    if result[\"rag_used\"]:\n",
    "                        temp = result.get('generation_temperature', 0.25)\n",
    "                        print(f\"  Generation: Temperature={temp}, Max Tokens={result['available_for_response']}\")\n",
    "                        print(f\"  Total Context Tokens Used: {result['context_tokens']} tokens\")\n",
    "                    else:\n",
    "                        temp = result.get('generation_temperature', 0.5)\n",
    "                        tokens = result.get('model_knowledge_tokens', 80)\n",
    "                        print(f\"  Generation: Temperature={temp}, Max Tokens={tokens}\")\n",
    "                        print(f\"  Reasoning: Cosine similarity below threshold, using model knowledge\")\n",
    "            \n",
    "            # Show token usage\n",
    "            if \"token_usage\" in result:\n",
    "                print(f\"\\nTOKEN USAGE (Context Window: {CONTEXT_WINDOW} tokens)\")\n",
    "                print(\"-\" * 80)\n",
    "                usage = result[\"token_usage\"]\n",
    "                print(f\"  Prompt Tokens: {usage.get('prompt_tokens', 'N/A')}\")\n",
    "                if usage.get('context_tokens', 0) > 0:\n",
    "                    print(f\"  Context Tokens: {usage['context_tokens']}\")\n",
    "                print(f\"  Answer Tokens: {usage.get('answer_tokens', 'N/A')}\")\n",
    "                print(f\"  Total Tokens: {usage.get('total_tokens', 'N/A')}\")\n",
    "                \n",
    "                total = usage.get('total_tokens', 0)\n",
    "                if total > CONTEXT_WINDOW:\n",
    "                    print(f\"  ⚠ WARNING: Exceeded context window by {total - CONTEXT_WINDOW} tokens!\")\n",
    "                else:\n",
    "                    remaining = CONTEXT_WINDOW - total\n",
    "                    print(f\"  Remaining: {remaining} tokens ({remaining/CONTEXT_WINDOW:.1%} free)\")\n",
    "            \n",
    "            print(\"\\nGENERATED ANSWER\")\n",
    "            print(\"-\" * 80)\n",
    "            print(result[\"answer\"])\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n\\n⚠ Session terminated by user (Ctrl+C)\")\n",
    "            print(f\"Total queries processed: {query_count}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query: {str(e)}\")\n",
    "            print(\"Please try again or type 'exit' to quit.\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# session run using mobile atlas\n",
    "if __name__ == \"__main__\":\n",
    "    run_interactive_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGROK SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T13:41:26.601771Z",
     "iopub.status.busy": "2026-02-05T13:41:26.601459Z",
     "iopub.status.idle": "2026-02-05T13:41:29.833693Z",
     "shell.execute_reply": "2026-02-05T13:41:29.832895Z",
     "shell.execute_reply.started": "2026-02-05T13:41:26.601740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install fastapi uvicorn pyngrok -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T13:41:29.835480Z",
     "iopub.status.busy": "2026-02-05T13:41:29.835188Z",
     "iopub.status.idle": "2026-02-05T13:41:30.510736Z",
     "shell.execute_reply": "2026-02-05T13:41:30.510049Z",
     "shell.execute_reply.started": "2026-02-05T13:41:29.835435Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NgrokTunnel: \"https://nikolas-interfilar-stalagmitically.ngrok-free.dev\" -> \"http://localhost:8000\"\n"
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "ngrok.set_auth_token(\"your_ngrok_auth_token_here\")\n",
    "\n",
    "public_url = ngrok.connect(8000)\n",
    "\n",
    "print(public_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T13:41:31.632238Z",
     "iopub.status.busy": "2026-02-05T13:41:31.631627Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING PRODUCTION API + NGROK (EXACT RAG LOGIC)\n",
      "================================================================================\n",
      "✓ API code created (EXACT RAG LOGIC WITH ALL IMPROVEMENTS)\n",
      "\n",
      "[1/2] Starting FastAPI server...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 13:41:37.503092: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770298897.525344     887 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770298897.531841     887 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770298897.548058     887 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770298897.548085     887 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770298897.548089     887 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770298897.548092     887 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "/kaggle/working/production_api.py:48: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n",
      "INFO:     Started server process [887]\n",
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INITIALIZING RAG SYSTEM\n",
      "================================================================================\n",
      "\n",
      "[1/3] Loading embedding model...\n",
      "✓ Embedding model ready\n",
      "[2/3] Loading FAISS index...\n",
      "✓ FAISS index loaded: {index.ntotal} vectors\n",
      "✓ Chunks loaded: {len(chunked_documents)} documents\n",
      "[3/3] Loading Atlas model (FP16)...\n",
      "Loading from: {ATLAS_MERGED_PATH}\n",
      "[2/2] Exposing with ngrok...\n",
      "\n",
      "================================================================================\n",
      "✅ API IS LIVE WITH ALL IMPROVEMENTS!\n",
      "================================================================================\n",
      "\n",
      "API URL: NgrokTunnel: \"https://nikolas-interfilar-stalagmitically.ngrok-free.dev\" -> \"http://localhost:8000\"\n",
      "Query: NgrokTunnel: \"https://nikolas-interfilar-stalagmitically.ngrok-free.dev\" -> \"http://localhost:8000\"/query\n",
      "Health: NgrokTunnel: \"https://nikolas-interfilar-stalagmitically.ngrok-free.dev\" -> \"http://localhost:8000\"/health\n",
      "Info: NgrokTunnel: \"https://nikolas-interfilar-stalagmitically.ngrok-free.dev\" -> \"http://localhost:8000\"/info\n",
      "\n",
      "================================================================================\n",
      "VERIFYING SYSTEM STATUS\n",
      "================================================================================\n",
      "⚠️ Health check failed: No connection adapters were found for 'NgrokTunnel: \"https://nikolas-interfilar-stalagmitically.ngrok-free.dev\" -> \"http://localhost:8000\"/health'\n",
      "\n",
      "================================================================================\n",
      "KEY IMPROVEMENTS ACTIVE:\n",
      "================================================================================\n",
      "✓ Threshold: 0.65 (was 0.78)\n",
      "✓ Dynamic tokens: 70-250 based on confidence\n",
      "✓ Anti-hallucination prompts with hidden reasoning\n",
      "✓ Temperature: 0.2-0.5 (lower for better accuracy)\n",
      "✓ Updated intent classification with 14 examples\n",
      "✓ Greeting similarity matching\n",
      "✓ Clean output (removes 'user:', 'assistant:' markers)\n",
      "✓ New thresholds: 0.77, 0.65, 0.45, 0.3\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Test with Postman or curl:\n",
      "POST NgrokTunnel: \"https://nikolas-interfilar-stalagmitically.ngrok-free.dev\" -> \"http://localhost:8000\"/query\n",
      "Body: {\"query\": \"واش علاش مهم نقلع عن التدخين؟\"}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "API running (Ctrl+C to stop)...\n",
      "✓ Atlas model loaded (FP16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     129.45.69.222:0 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     129.45.69.222:0 - \"POST /query HTTP/1.1\" 200 OK\n",
      "INFO:     129.45.69.222:0 - \"POST /query HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING PRODUCTION API + NGROK (EXACT RAG LOGIC)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save the corrected API code with actual path values injected\n",
    "api_code = '''\"\"\"\n",
    "Production RAG API for Kaggle + ngrok\n",
    "Uses EXACT logic from your fine-tuned Atlas model with all improvements\n",
    "\"\"\"\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Configuration - Match your RAG code exactly\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "CONTEXT_WINDOW = 1024\n",
    "CONFIDENCE_THRESHOLD = 0.65  # UPDATED from 0.78\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_CONTEXT_LENGTH = 512\n",
    "\n",
    "# Paths injected from notebook variables\n",
    "INDEX_PATH = \"{index_path}\"\n",
    "CHUNKS_PATH = \"{chunks_path}\"\n",
    "ATLAS_MERGED_PATH = \"{atlas_merged_path}\"\n",
    "\n",
    "app = FastAPI(title=\"Atlas RAG API\", version=\"2.0.0\")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Global state\n",
    "embed_model = None\n",
    "atlas_model = None\n",
    "atlas_tokenizer = None\n",
    "index = None\n",
    "chunked_documents = []\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    global embed_model, atlas_model, atlas_tokenizer, index, chunked_documents\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80, flush=True)\n",
    "    print(\"INITIALIZING RAG SYSTEM\", flush=True)\n",
    "    print(\"=\"*80, flush=True)\n",
    "    \n",
    "    # Load embedding model\n",
    "    print(\"\\\\n[1/3] Loading embedding model...\", flush=True)\n",
    "    try:\n",
    "        embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "        print(\"✓ Embedding model ready\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {{e}}\", flush=True)\n",
    "    \n",
    "    # Load FAISS index\n",
    "    print(\"[2/3] Loading FAISS index...\", flush=True)\n",
    "    try:\n",
    "        index_path = INDEX_PATH\n",
    "        chunks_path = CHUNKS_PATH\n",
    "        \n",
    "        if Path(index_path).exists() and Path(chunks_path).exists():\n",
    "            index = faiss.read_index(index_path)\n",
    "            with open(chunks_path, \"rb\") as f:\n",
    "                chunked_documents = pickle.load(f)\n",
    "            print(f\"✓ FAISS index loaded: {{index.ntotal}} vectors\", flush=True)\n",
    "            print(f\"✓ Chunks loaded: {{len(chunked_documents)}} documents\", flush=True)\n",
    "        else:\n",
    "            print(\"⚠ FAISS files not found\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {{e}}\", flush=True)\n",
    "    \n",
    "    # Load Atlas model (FP16)\n",
    "    print(\"[3/3] Loading Atlas model (FP16)...\", flush=True)\n",
    "    try:\n",
    "        if ATLAS_MERGED_PATH and ATLAS_MERGED_PATH != \"\":\n",
    "            print(f\"Loading from: {{ATLAS_MERGED_PATH}}\", flush=True)\n",
    "            atlas_model = AutoModelForCausalLM.from_pretrained(\n",
    "                str(ATLAS_MERGED_PATH),\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            atlas_tokenizer = AutoTokenizer.from_pretrained(\n",
    "                str(ATLAS_MERGED_PATH),\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            if atlas_tokenizer.pad_token is None:\n",
    "                atlas_tokenizer.pad_token = atlas_tokenizer.eos_token\n",
    "            \n",
    "            print(\"✓ Atlas model loaded (FP16)\", flush=True)\n",
    "        else:\n",
    "            print(f\"⚠ Model path not provided: {{ATLAS_MERGED_PATH}}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading model: {{e}}\", flush=True)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    sys.stdout.flush()\n",
    "\n",
    "def count_tokens_exact(text, model_type=\"atlas\"):\n",
    "    \"\"\"Count tokens exactly - match your RAG code\"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        if atlas_tokenizer:\n",
    "            return len(atlas_tokenizer.encode(text))\n",
    "        \n",
    "        import tiktoken\n",
    "        try:\n",
    "            encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "            return len(encoder.encode(text))\n",
    "        except:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    words = len(text.split())\n",
    "    return int(words / 0.75)\n",
    "\n",
    "def embed_query_simple(text):\n",
    "    \"\"\"Simple embedding function - match your RAG code\"\"\"\n",
    "    try:\n",
    "        embedding = embed_model.encode(text, convert_to_numpy=True)\n",
    "        embedding = embedding / np.linalg.norm(embedding)\n",
    "        return embedding.astype(\"float32\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def retrieve_documents(question, top_k=5, max_tokens=None):\n",
    "    \"\"\"Retrieve documents with token budget - match your RAG code\"\"\"\n",
    "    if index is None or not chunked_documents:\n",
    "        return []\n",
    "    \n",
    "    if max_tokens is None:\n",
    "        max_tokens = 500\n",
    "    \n",
    "    query_vec = embed_query_simple(question)\n",
    "    if query_vec is None:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        scores, indices = index.search(np.array([query_vec]), min(top_k * 2, len(chunked_documents)))\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        if idx < 0 or idx >= len(chunked_documents):\n",
    "            continue\n",
    "        \n",
    "        chunk = chunked_documents[idx]\n",
    "        tokens = chunk[\"metadata\"].get(\"token_count\", count_tokens_exact(chunk[\"text\"]))\n",
    "        \n",
    "        if results and total_tokens + tokens > max_tokens:\n",
    "            break\n",
    "        \n",
    "        results.append({\n",
    "            \"text\": chunk[\"text\"],\n",
    "            \"metadata\": chunk[\"metadata\"],\n",
    "            \"score\": float(score),\n",
    "            \"tokens\": tokens\n",
    "        })\n",
    "        total_tokens += tokens\n",
    "        \n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "    \n",
    "    return sorted(results, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "def interpret_cosine_similarity(score):\n",
    "    \"\"\"UPDATED interpretation with new thresholds - match your RAG code exactly\"\"\"\n",
    "    angle = np.degrees(np.arccos(min(max(score, 0.0), 1.0)))\n",
    "    \n",
    "    if score >= 0.65:\n",
    "        category = \"VERY STRONG MATCH\"\n",
    "        confidence = \"Very High\"\n",
    "        angle_desc = f\"{angle:.1f}° (almost identical direction)\"\n",
    "        action = \"RAG with high confidence\"\n",
    "    elif score >= 0.5:  # NEW THRESHOLD\n",
    "        category = \"STRONG MATCH\"\n",
    "        confidence = \"High\"\n",
    "        angle_desc = f\"{angle:.1f}° (very close)\"\n",
    "        action = \"RAG recommended\"\n",
    "    elif score >= 0.45:\n",
    "        category = \"GOOD MATCH\"\n",
    "        confidence = \"Medium-High\"\n",
    "        angle_desc = f\"{angle:.1f}° (good similarity)\"\n",
    "        action = \"RAG suitable\"\n",
    "    elif score >= 0.35:\n",
    "        category = \"MODERATE MATCH\"\n",
    "        confidence = \"Medium\"\n",
    "        angle_desc = f\"{angle:.1f}° (somewhat related)\"\n",
    "        action = \"Consider RAG\"\n",
    "    elif score >= 0.3:\n",
    "        category = \"WEAK MATCH\"\n",
    "        confidence = \"Low\"\n",
    "        angle_desc = f\"{angle:.1f}° (weak relation)\"\n",
    "        action = \"Model knowledge preferred\"\n",
    "    elif score >= 0.25:\n",
    "        category = \"VERY WEAK MATCH\"\n",
    "        confidence = \"Very Low\"\n",
    "        angle_desc = f\"{angle:.1f}° (barely related)\"\n",
    "        action = \"Model knowledge recommended\"\n",
    "    else:\n",
    "        category = \"POOR MATCH\"\n",
    "        confidence = \"None\"\n",
    "        angle_desc = f\"{angle:.1f}° (unrelated)\"\n",
    "        action = \"Use model knowledge only\"\n",
    "    \n",
    "    return {\n",
    "        \"category\": category,\n",
    "        \"confidence_level\": confidence,\n",
    "        \"angle_degrees\": round(angle, 1),\n",
    "        \"angle_description\": angle_desc,\n",
    "        \"recommended_action\": action,\n",
    "        \"score\": score\n",
    "    }\n",
    "\n",
    "def classify_intent_hybrid(question):\n",
    "    \"\"\"UPDATED with new examples and logic - match your RAG code\"\"\"\n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    # Fast rule-based filter\n",
    "    greeting_keywords = [\n",
    "        \"سلام\", \"السلام\", \"مرحبا\", \"صباح\", \"مساء\", \n",
    "        \"شكرا\", \"شكراً\", \"أهلا\", \"اهلا\", \"واش راك\",\n",
    "        \"واش حالك\", \"واش كيفك\", \"كيف راك\", \"كيفاش راك\",\n",
    "        \"واش الحالة\", \"كيف حالك\", \"كيف الحال\"\n",
    "    ]\n",
    "    \n",
    "    words = question_lower.split()\n",
    "    if len(words) <= 4:  # UPDATED from 3 to 4\n",
    "        if any(kw in question_lower for kw in greeting_keywords):\n",
    "            return \"تحية\", generate_special_response(\"تحية\", question)\n",
    "    \n",
    "    # UPDATED few-shot examples with new patterns\n",
    "    few_shot_examples = [\n",
    "        # GREETINGS (Adding slang)\n",
    "        (\"واش الحالة\", \"تحية\"),\n",
    "        (\"صحة خويا\", \"تحية\"),\n",
    "        (\"واش يا البوت\", \"تحية\"),\n",
    "        (\"واش راك داير فيها\", \"تحية\"),\n",
    "        \n",
    "        # SMOKING (Adding frustration/emotions)\n",
    "        (\"كرهت حياتي ياخو\", \"سؤال تدخين\"),\n",
    "        (\"غلبتني السيجارة\", \"سؤال تدخين\"),\n",
    "        (\"راني فشلان وتعبان\", \"سؤال تدخين\"),\n",
    "        (\"حاب نتهنى من هاد السم\", \"سؤال تدخين\"),\n",
    "        \n",
    "        # ACTUAL INSULTS (Be very specific)\n",
    "        (\"أنت حمار\", \"سب\"),\n",
    "        (\"تفو عليك\", \"سب\"),\n",
    "        (\"يا ولد الحرام\", \"سب\"),\n",
    "        \n",
    "        # ACTUAL OFF-TOPIC\n",
    "        (\"كيفاش نطيب اللحم؟\", \"غير ذي صلة\"),\n",
    "        (\"شكون ربح الماتش؟\", \"غير ذي صلة\"),\n",
    "        (\"واش رايك في ميسي؟\", \"غير ذي صلة\")\n",
    "    ]\n",
    "    \n",
    "    examples_text = \"\\\\n\".join([f\"الرسالة: {q}\\\\nالتصنيف: {c}\" for q, c in few_shot_examples])\n",
    "    \n",
    "    classifier_prompt = f\"\"\"تصنف الرسالة في واحد من: سؤال تدخين / تحية / سب / غير ذي صلة\n",
    "\n",
    "أمثلة:\n",
    "{examples_text}\n",
    "\n",
    "الرسالة: {question}\n",
    "التصنيف:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        inputs = atlas_tokenizer(\n",
    "            classifier_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = atlas_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=15,\n",
    "                temperature=0.1,\n",
    "                top_p=0.5,\n",
    "                do_sample=True,\n",
    "                pad_token_id=atlas_tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        intent_raw = atlas_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if \"التصنيف:\" in intent_raw:\n",
    "            intent_raw = intent_raw.split(\"التصنيف:\")[-1].strip()\n",
    "        \n",
    "        scores = {\n",
    "            \"تحية\": intent_raw.count(\"تحية\"),\n",
    "            \"سب\": intent_raw.count(\"سب\"),\n",
    "            \"غير ذي صلة\": intent_raw.count(\"غير ذي صلة\"),\n",
    "            \"سؤال تدخين\": intent_raw.count(\"سؤال تدخين\")\n",
    "        }\n",
    "        \n",
    "        top_intent = max(scores, key=scores.get)\n",
    "        \n",
    "        if top_intent == \"تحية\" or scores[\"تحية\"] > 0:\n",
    "            return \"greeting\", generate_special_response(top_intent, question)\n",
    "        elif top_intent == \"سب\" or scores[\"سب\"] > 0:\n",
    "            return \"insult\", \"معليش، نحترم الجميع هنا. عندك سؤال حول التدخين؟\"\n",
    "        elif top_intent == \"غير ذي صلة\" or scores[\"غير ذي صلة\"] > 0:\n",
    "            return \"off_topic\", \"خاطيني، أنا نجاوب غير على أسئلة التدخين والإقلاع عنه.\"\n",
    "        \n",
    "        return \"smoking\", None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Intent classification error: {e}\", flush=True)\n",
    "        return \"smoking\", None\n",
    "\n",
    "def select_greeting_by_similarity(user_question):\n",
    "    \"\"\"Select greeting by similarity - match your RAG code\"\"\"\n",
    "    greeting_map = [\n",
    "        {\"anchors\": [\"السلام عليكم\", \"سلام\"], \"response\": \"وعليكم السلام ورحمة الله! مرحبا بيك، قولي واش هو سؤالك على التدخين؟\"},\n",
    "        {\"anchors\": [\"أهلا\", \"مرحبا\", \"واش راك\"], \"response\": \"أهلا بيك! واش راك؟ كيفاش نقدر نعاونك اليوم في موضوع التدخين؟\"},\n",
    "        {\"anchors\": [\"صباح الخير\", \"كي صبحت\"], \"response\": \"صباح النور والسرور! واش راك؟ كاش ما نقدر نعاونك في موضوع التدخين اليوم؟\"},\n",
    "        {\"anchors\": [\"مساء الخير\", \"كي عشيت\"], \"response\": \"مساء الخير والأنوار! واش أحوالك؟ راني هنا إذا سحقيت كاش نصيحة على التدخين\"}\n",
    "    ]\n",
    "    \n",
    "    anchor_phrases = [m[\"anchors\"][0] for m in greeting_map]\n",
    "    responses = [m[\"response\"] for m in greeting_map]\n",
    "\n",
    "    user_embedding = embed_query_simple(user_question)\n",
    "    if user_embedding is None:\n",
    "        return responses[1], 0.0\n",
    "    \n",
    "    greeting_embeddings = []\n",
    "    for anchor in anchor_phrases:\n",
    "        emb = embed_query_simple(anchor)\n",
    "        if emb is not None:\n",
    "            greeting_embeddings.append(emb)\n",
    "        else:\n",
    "            greeting_embeddings.append(np.zeros_like(user_embedding))\n",
    "    \n",
    "    greeting_embeddings = np.array(greeting_embeddings)\n",
    "    similarities = np.dot(greeting_embeddings, user_embedding)\n",
    "    \n",
    "    best_idx = np.argmax(similarities)\n",
    "    best_similarity = float(similarities[best_idx])\n",
    "    selected_greeting = responses[best_idx]\n",
    "    \n",
    "    return selected_greeting, best_similarity\n",
    "\n",
    "def generate_special_response(query_type_info, question):\n",
    "    \"\"\"Generate special response - match your RAG code\"\"\"\n",
    "    if isinstance(query_type_info, str):\n",
    "        q_type = query_type_info\n",
    "        if query_type_info == \"تحية\":\n",
    "            selected, similarity = select_greeting_by_similarity(question)\n",
    "            return selected\n",
    "        elif query_type_info == \"سب\":\n",
    "            return \"معليش، ما نقدرش نجاوب على هاد النوع من الكلام. تكلم باحترام ونقدر نعاونك.\"\n",
    "    else:\n",
    "        q_type = query_type_info.get(\"type\")\n",
    "        if query_type_info[\"type\"] == \"تحية\":\n",
    "            selected, similarity = select_greeting_by_similarity(question)\n",
    "            return selected\n",
    "        elif query_type_info[\"type\"] == \"سب\":\n",
    "            return \"معليش، ما نقدرش نجاوب على هاد النوع من الكلام. تكلم باحترام ونقدر نعاونك.\"\n",
    "    return None\n",
    "\n",
    "def clean_model_output(answer):\n",
    "    \"\"\"\n",
    "    Clean model output to remove any training markers like user:, assistant:, system:\n",
    "    This fixes the issue where model echoes back its training format.\n",
    "    \"\"\"\n",
    "    answer = answer.strip()\n",
    "    \n",
    "    # Pattern 1: Remove \"user:\" or \"assistant:\" prefixes\n",
    "    answer = re.sub(r'^(user:|assistant:|system:)\\\\s*', '', answer, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Pattern 2: If there's \"A: \" at the start (from our prompt), remove it\n",
    "    if answer.startswith('A: '):\n",
    "        answer = answer[3:].strip()\n",
    "    \n",
    "    # Pattern 3: Remove any conversation format that leaked through\n",
    "    markers = ['\\\\nuser:', '\\\\nassistant:', '\\\\nsystem:', '\\\\nA:', '\\\\nالسياق:', '\\\\nالسؤال:']\n",
    "    for marker in markers:\n",
    "        if marker in answer:\n",
    "            answer = answer.split(marker)[0].strip()\n",
    "    \n",
    "    # Pattern 4: If answer still has system/user/assistant anywhere, extract after last marker\n",
    "    if any(word in answer.lower() for word in ['user:', 'assistant:', 'system:']):\n",
    "        parts = re.split(r'(user:|assistant:|system:|A:)', answer, flags=re.IGNORECASE)\n",
    "        if len(parts) > 1:\n",
    "            for part in reversed(parts):\n",
    "                if part.strip() and not any(m in part.lower() for m in ['user:', 'assistant:', 'system:']):\n",
    "                    answer = part.strip()\n",
    "                    break\n",
    "    \n",
    "    return answer.strip()\n",
    "\n",
    "def generate_answer(question, context=None, max_tokens_for_response=120, use_rag=True, temperature=None):\n",
    "    \"\"\"UPDATED generate_answer with anti-hallucination measures - match your RAG code exactly\"\"\"\n",
    "    \n",
    "    if use_rag and context:\n",
    "        # UPDATED with anti-hallucination prompt\n",
    "        system_prompt = (\n",
    "            \"أنت خبير جزائري مختص في التوعية ضد التدخين. \"\n",
    "            \"تحدث بالدارجة الجزائرية البيضاء (فصيحة تقنياً). \"\n",
    "            \"التزم بالحقائق العلمية فقط. ممنوع الدراما، ممنوع السياسة، وممنوع التحدث عن دول أخرى .جاوب مباشرة تقنياً. ابدأ الجواب بـ 'بناءً على المعلومات المتوفرة'\"\n",
    "            \"خاطب المستخدم بصيغة المذكر دائماً إلا إذا ذكر عكس ذلك.\"\n",
    "        )\n",
    "        # UPDATED: Anti-hallucination with hidden reasoning\n",
    "        prompt = f\"\"\"system: {system_prompt}\n",
    "\n",
    "{context if context else \"ملاحظة: لا يوجد سياق خارجي، جاوب باختصار من القواعد العامة للإقلاع عن التدخين.\"}\n",
    "\n",
    "التعليمات الإجبارية:\n",
    "1. استخرج النصائح من السياق (Context) إذا كان متوفراً.\n",
    "2. إذا كان السؤال بعيداً عن التدخين، قل: \"أنا هنا للمساعدة في الإقلاع عن التدخين فقط\".\n",
    "3. ممنوع نهائياً ذكر أي جمل درامية غير واقعية.\n",
    "4. الجواب يكون في شكل نقاط (Bullet points) ليكون واضحاً.\n",
    "5. لا تزد عن 80 كلمة لتجنب انقطاع النص.\n",
    "\n",
    "user: السؤال: {question}\n",
    "\n",
    "A: \"\"\"\n",
    "        \n",
    "        if temperature is None:\n",
    "            temperature = 0.2  # UPDATED from 0.35\n",
    "    else:\n",
    "        system_prompt = (\n",
    "            \"أنت مساعد جزائري مختص في التدخين والإقلاع عنه، تهدر بالدارجة الجزائرية. \"\n",
    "            \"جاوب بإجابات قصيرة ومباشرة وعملية بلا خطبة. ممنوع تمد نصائح طبية من راسك. ممنوع الفلسفة.\"\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"system: {system_prompt}\n",
    "\n",
    "user: {question}\n",
    "\n",
    "A: \"\"\"\n",
    "        \n",
    "        if temperature is None:\n",
    "            temperature = 0.4  # UPDATED from 0.7\n",
    "    \n",
    "    try:\n",
    "        inputs = atlas_tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_CONTEXT_LENGTH\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = atlas_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens_for_response,\n",
    "                temperature=temperature,\n",
    "                top_p=0.4,              # UPDATED from 0.8\n",
    "                top_k=30,               # UPDATED from 40\n",
    "                do_sample=True,\n",
    "                repetition_penalty=1.2,  # UPDATED from 1.15\n",
    "                pad_token_id=atlas_tokenizer.eos_token_id,\n",
    "                eos_token_id=atlas_tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Extract only the new tokens generated (not the prompt)\n",
    "        generated_tokens = outputs[0][prompt_length:]\n",
    "        raw_answer = atlas_tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Clean the output to remove any training format artifacts\n",
    "        answer = clean_model_output(raw_answer)\n",
    "        \n",
    "        prompt_tokens = count_tokens_exact(prompt)\n",
    "        answer_tokens = count_tokens_exact(answer)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"total_prompt_tokens\": prompt_tokens,\n",
    "            \"answer_tokens\": answer_tokens,\n",
    "            \"context_tokens_used\": count_tokens_exact(context) if context else 0,\n",
    "            \"total_tokens\": prompt_tokens + answer_tokens\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer: {str(e)}\", flush=True)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            \"answer\": \"معليش، وقعت مشكلة في الجواب. حاول تاني بعد شوية.\",\n",
    "            \"total_prompt_tokens\": 0,\n",
    "            \"answer_tokens\": 0,\n",
    "            \"context_tokens_used\": 0,\n",
    "            \"total_tokens\": 0\n",
    "        }\n",
    "\n",
    "def process_query(question):\n",
    "    \"\"\"UPDATED complete RAG pipeline with new thresholds - match your RAG code exactly\"\"\"\n",
    "    \n",
    "    # STEP 1: Pre-RAG Intent Classification\n",
    "    intent, canned_response = classify_intent_hybrid(question)\n",
    "    \n",
    "    if canned_response:\n",
    "        return {\n",
    "            \"answer\": canned_response,\n",
    "            \"confidence\": 1.0,\n",
    "            \"selected_document\": None,\n",
    "            \"all_documents\": [],\n",
    "            \"context_tokens\": 0,\n",
    "            \"rag_used\": False,\n",
    "            \"query_type\": intent,\n",
    "            \"reason\": f\"Intent classification: {intent} (handled pre-RAG)\",\n",
    "            \"similarity_interpretation\": None,\n",
    "            \"special_handling\": True,\n",
    "            \"token_usage\": {\n",
    "                \"prompt_tokens\": count_tokens_exact(f\"system: \\\\nuser: {question}\\\\nassistant: \"),\n",
    "                \"answer_tokens\": count_tokens_exact(canned_response),\n",
    "                \"total_tokens\": count_tokens_exact(f\"system: \\\\nuser: {question}\\\\nassistant: {canned_response}\"),\n",
    "                \"context_window\": CONTEXT_WINDOW\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # STEP 2: Retrieve documents\n",
    "    retrieved = retrieve_documents(question, top_k=5, max_tokens=500)\n",
    "    \n",
    "    if not retrieved:\n",
    "        result = generate_answer(question, context=None, max_tokens_for_response=80, use_rag=False)\n",
    "        return {\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"confidence\": 0.0,\n",
    "            \"selected_document\": None,\n",
    "            \"all_documents\": [],\n",
    "            \"context_tokens\": 0,\n",
    "            \"rag_used\": False,\n",
    "            \"query_type\": \"smoking\",\n",
    "            \"reason\": \"No relevant documents found\",\n",
    "            \"similarity_interpretation\": interpret_cosine_similarity(0.0),\n",
    "            \"token_usage\": {\n",
    "                \"prompt_tokens\": result.get(\"total_prompt_tokens\", 0),\n",
    "                \"answer_tokens\": result.get(\"answer_tokens\", 0),\n",
    "                \"total_tokens\": result.get(\"total_tokens\", 0),\n",
    "                \"context_window\": CONTEXT_WINDOW\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    best_doc = retrieved[0]\n",
    "    best_score = best_doc[\"score\"]\n",
    "    best_title = best_doc[\"metadata\"].get(\"title\", \"N/A\")\n",
    "    context = best_doc[\"text\"]\n",
    "    \n",
    "    similarity_interpretation = interpret_cosine_similarity(best_score)\n",
    "    \n",
    "    # STEP 3: UPDATED dynamic token allocation with new thresholds\n",
    "    if best_score >= 0.77:  # NEW THRESHOLD\n",
    "        max_tokens_for_response = 250\n",
    "        temperature = 0.32\n",
    "    elif best_score >= CONFIDENCE_THRESHOLD:  # 0.65\n",
    "        max_tokens_for_response = 180\n",
    "        temperature = 0.4\n",
    "    elif best_score >= 0.45:  # NEW THRESHOLD\n",
    "        max_tokens_for_response = 150\n",
    "        temperature = 0.45\n",
    "        # Concatenate all docs\n",
    "        context_parts = []\n",
    "        for doc in retrieved:\n",
    "            context_parts.append(f\"[{doc['metadata']['title']}]\\\\n{doc['text']}\")\n",
    "        context = \"\\\\n---\\\\n\".join(context_parts)\n",
    "        \n",
    "        context_tokens_used = sum(doc[\"tokens\"] for doc in retrieved)\n",
    "        \n",
    "        result = generate_answer(question, context, max_tokens_for_response=max_tokens_for_response, \n",
    "                               use_rag=True, temperature=temperature)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"confidence\": round(best_score, 4),\n",
    "            \"selected_document\": {\n",
    "                \"rank\": 1,\n",
    "                \"title\": best_title,\n",
    "                \"score\": best_score,\n",
    "                \"tokens\": best_doc.get(\"tokens\", 0)\n",
    "            },\n",
    "            \"all_documents\": [\n",
    "                {\n",
    "                    \"rank\": i + 1,\n",
    "                    \"title\": doc[\"metadata\"].get(\"title\", \"N/A\"),\n",
    "                    \"score\": round(doc[\"score\"], 4),\n",
    "                    \"tokens\": doc.get(\"tokens\", 0)\n",
    "                }\n",
    "                for i, doc in enumerate(retrieved)\n",
    "            ],\n",
    "            \"context_tokens\": context_tokens_used,\n",
    "            \"available_for_response\": max_tokens_for_response,\n",
    "            \"rag_used\": True,\n",
    "            \"query_type\": \"smoking\",\n",
    "            \"reason\": f\"Good match RAG ({best_score:.4f} ≥ 0.45)\",\n",
    "            \"similarity_interpretation\": similarity_interpretation,\n",
    "            \"generation_temperature\": temperature,\n",
    "            \"token_usage\": {\n",
    "                \"prompt_tokens\": result.get(\"total_prompt_tokens\", 0),\n",
    "                \"answer_tokens\": result.get(\"answer_tokens\", 0),\n",
    "                \"context_tokens\": result.get(\"context_tokens_used\", 0),\n",
    "                \"total_tokens\": result.get(\"total_tokens\", 0),\n",
    "                \"context_window\": CONTEXT_WINDOW\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        # Confidence too low, use model knowledge with UPDATED parameters\n",
    "        if best_score >= 0.3:\n",
    "            temperature = 0.45\n",
    "            max_tokens = 80\n",
    "        else:\n",
    "            temperature = 0.5\n",
    "            max_tokens = 70\n",
    "        \n",
    "        result = generate_answer(question, context=None, max_tokens_for_response=max_tokens, \n",
    "                               use_rag=False, temperature=temperature)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"confidence\": round(best_score, 4),\n",
    "            \"selected_document\": {\n",
    "                \"rank\": 1,\n",
    "                \"title\": best_title,\n",
    "                \"score\": best_score,\n",
    "                \"tokens\": best_doc.get(\"tokens\", 0)\n",
    "            },\n",
    "            \"all_documents\": [\n",
    "                {\n",
    "                    \"rank\": i + 1,\n",
    "                    \"title\": doc[\"metadata\"].get(\"title\", \"N/A\"),\n",
    "                    \"score\": round(doc[\"score\"], 4),\n",
    "                    \"tokens\": doc.get(\"tokens\", 0)\n",
    "                }\n",
    "                for i, doc in enumerate(retrieved)\n",
    "            ],\n",
    "            \"context_tokens\": 0,\n",
    "            \"rag_used\": False,\n",
    "            \"query_type\": \"smoking\",\n",
    "            \"reason\": f\"Cosine similarity {best_score:.4f} < {CONFIDENCE_THRESHOLD} (threshold)\",\n",
    "            \"similarity_interpretation\": similarity_interpretation,\n",
    "            \"generation_temperature\": temperature,\n",
    "            \"model_knowledge_tokens\": max_tokens,\n",
    "            \"token_usage\": {\n",
    "                \"prompt_tokens\": result.get(\"total_prompt_tokens\", 0),\n",
    "                \"answer_tokens\": result.get(\"answer_tokens\", 0),\n",
    "                \"total_tokens\": result.get(\"total_tokens\", 0),\n",
    "                \"context_window\": CONTEXT_WINDOW\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # High confidence RAG path (>= 0.77)\n",
    "    result = generate_answer(question, context, max_tokens_for_response=max_tokens_for_response, \n",
    "                           use_rag=True, temperature=temperature)\n",
    "    \n",
    "    return {\n",
    "        \"answer\": result[\"answer\"],\n",
    "        \"confidence\": round(best_score, 4),\n",
    "        \"selected_document\": {\n",
    "            \"rank\": 1,\n",
    "            \"title\": best_title,\n",
    "            \"score\": best_score,\n",
    "            \"tokens\": best_doc.get(\"tokens\", 0)\n",
    "        },\n",
    "        \"all_documents\": [\n",
    "            {\n",
    "                \"rank\": i + 1,\n",
    "                \"title\": doc[\"metadata\"].get(\"title\", \"N/A\"),\n",
    "                \"score\": round(doc[\"score\"], 4),\n",
    "                \"tokens\": doc.get(\"tokens\", 0)\n",
    "            }\n",
    "            for i, doc in enumerate(retrieved)\n",
    "        ],\n",
    "        \"context_tokens\": result.get(\"context_tokens_used\", 0),\n",
    "        \"available_for_response\": max_tokens_for_response,\n",
    "        \"rag_used\": True,\n",
    "        \"query_type\": \"smoking\",\n",
    "        \"reason\": f\"High confidence RAG ({best_score:.4f} ≥ {CONFIDENCE_THRESHOLD})\",\n",
    "        \"similarity_interpretation\": similarity_interpretation,\n",
    "        \"generation_temperature\": temperature,\n",
    "        \"token_usage\": {\n",
    "            \"prompt_tokens\": result.get(\"total_prompt_tokens\", 0),\n",
    "            \"answer_tokens\": result.get(\"answer_tokens\", 0),\n",
    "            \"context_tokens\": result.get(\"context_tokens_used\", 0),\n",
    "            \"total_tokens\": result.get(\"total_tokens\", 0),\n",
    "            \"context_window\": CONTEXT_WINDOW\n",
    "        }\n",
    "    }\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"status\": \"online\", \"service\": \"Atlas RAG API v2.0\"}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\n",
    "        \"status\": \"healthy\", \n",
    "        \"documents\": len(chunked_documents), \n",
    "        \"model_loaded\": atlas_model is not None,\n",
    "        \"tokenizer_loaded\": atlas_tokenizer is not None,\n",
    "        \"index_loaded\": index is not None,\n",
    "        \"model\": \"Atlas 2B FP16\",\n",
    "        \"threshold\": CONFIDENCE_THRESHOLD,\n",
    "        \"context_window\": CONTEXT_WINDOW\n",
    "    }\n",
    "\n",
    "@app.post(\"/query\")\n",
    "async def query(request: QueryRequest):\n",
    "    try:\n",
    "        return process_query(request.query)\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error in /query endpoint: {str(e)}\", flush=True)\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": str(e), \"answer\": \"معليش، صار خلل. حاول مرة أخرى.\"}\n",
    "\n",
    "@app.get(\"/info\")\n",
    "async def info():\n",
    "    return {\n",
    "        \"model\": \"Atlas 2B FP16\",\n",
    "        \"documents\": len(chunked_documents),\n",
    "        \"threshold\": CONFIDENCE_THRESHOLD,\n",
    "        \"context_window\": CONTEXT_WINDOW,\n",
    "        \"version\": \"2.0.0 (All improvements included)\",\n",
    "        \"model_loaded\": atlas_model is not None,\n",
    "        \"components\": {\n",
    "            \"embedding\": embed_model is not None,\n",
    "            \"faiss\": index is not None,\n",
    "            \"atlas_model\": atlas_model is not None,\n",
    "            \"tokenizer\": atlas_tokenizer is not None\n",
    "        }\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "'''\n",
    "\n",
    "# Inject the actual path values using string replacement\n",
    "api_code_with_paths = api_code.replace(\"{index_path}\", INDEX_PATH)\n",
    "api_code_with_paths = api_code_with_paths.replace(\"{chunks_path}\", CHUNKS_PATH)\n",
    "api_code_with_paths = api_code_with_paths.replace(\"{atlas_merged_path}\", ATLAS_MERGED_PATH)\n",
    "\n",
    "# Write API code\n",
    "with open(\"production_api.py\", \"w\") as f:\n",
    "    f.write(api_code_with_paths)\n",
    "\n",
    "print(\"✓ API code created (EXACT RAG LOGIC WITH ALL IMPROVEMENTS)\")\n",
    "\n",
    "# Start FastAPI WITHOUT piping stdout\n",
    "print(\"\\n[1/2] Starting FastAPI server...\")\n",
    "api_process = subprocess.Popen(\n",
    "    [\"python\", \"-u\", \"production_api.py\"],  # -u for unbuffered output\n",
    "    stdout=None,  # Don't pipe - let output show\n",
    "    stderr=None\n",
    ")\n",
    "\n",
    "time.sleep(15)  # Give more time for model loading\n",
    "\n",
    "# Expose with ngrok\n",
    "print(\"[2/2] Exposing with ngrok...\")\n",
    "try:\n",
    "    public_url = ngrok.connect(8000)\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ API IS LIVE WITH ALL IMPROVEMENTS!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nAPI URL: {public_url}\")\n",
    "    print(f\"Query: {public_url}/query\")\n",
    "    print(f\"Health: {public_url}/health\")\n",
    "    print(f\"Info: {public_url}/info\")\n",
    "    \n",
    "    # Test health endpoint\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VERIFYING SYSTEM STATUS\")\n",
    "    print(\"=\"*80)\n",
    "    try:\n",
    "        health_response = requests.get(f\"{public_url}/health\", timeout=10)\n",
    "        health_data = health_response.json()\n",
    "        print(f\"✓ Health check successful:\")\n",
    "        print(f\"  Documents loaded: {health_data.get('documents', 0)}\")\n",
    "        print(f\"  Model loaded: {health_data.get('model_loaded', False)}\")\n",
    "        print(f\"  Tokenizer loaded: {health_data.get('tokenizer_loaded', False)}\")\n",
    "        print(f\"  Index loaded: {health_data.get('index_loaded', False)}\")\n",
    "        \n",
    "        if not health_data.get('model_loaded'):\n",
    "            print(\"\\n⚠️ WARNING: Atlas model failed to load!\")\n",
    "        if health_data.get('documents', 0) == 0:\n",
    "            print(\"\\n⚠️ WARNING: No documents loaded!\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Health check failed: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"KEY IMPROVEMENTS ACTIVE:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"✓ Threshold: 0.65 (was 0.78)\")\n",
    "    print(\"✓ Dynamic tokens: 70-250 based on confidence\")\n",
    "    print(\"✓ Anti-hallucination prompts with hidden reasoning\")\n",
    "    print(\"✓ Temperature: 0.2-0.5 (lower for better accuracy)\")\n",
    "    print(\"✓ Updated intent classification with 14 examples\")\n",
    "    print(\"✓ Greeting similarity matching\")\n",
    "    print(\"✓ Clean output (removes 'user:', 'assistant:' markers)\")\n",
    "    print(\"✓ New thresholds: 0.77, 0.65, 0.45, 0.3\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Test with Postman or curl:\")\n",
    "    print(f\"POST {public_url}/query\")\n",
    "    print('Body: {\"query\": \"واش علاش مهم نقلع عن التدخين؟\"}')\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Keep running\n",
    "    print(\"\\nAPI running (Ctrl+C to stop)...\")\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(30)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopping...\")\n",
    "        ngrok.disconnect(public_url)\n",
    "        api_process.terminate()\n",
    "        print(\"✓ Stopped\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n",
    "    api_process.terminate()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9405295,
     "sourceId": 14719989,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9408282,
     "sourceId": 14724183,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9408323,
     "sourceId": 14724235,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9408383,
     "sourceId": 14724314,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
